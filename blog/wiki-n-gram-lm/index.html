<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>Creating a n-gram Language Model using Wikipedia</title>

    <!-- Open Graph Meta -->
    <meta content="Dani's Braindump" property="og:site_name">
    
      <meta content="Creating a n-gram Language Model using Wikipedia" property="og:title">
    
    
      <meta content="article" property="og:type">
    
    
      <meta content="My thoughts about the world" property="og:description">
    
    
      <meta content="http://localhost:4000/blog/wiki-n-gram-lm/" property="og:url">
    
    
      <meta content="2018-09-27T00:00:00+02:00" property="article:published_time">
      <meta content="http://localhost:4000/about/" property="article:author">
    
    
      <meta content="http://localhost:4000/assets/img/header_image.jpg" property="og:image">
    

    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@danitiefenauer">
    <meta name="twitter:creator" content="@danitiefenauer">
    
      <meta name="twitter:title" content="Creating a n-gram Language Model using Wikipedia">
    
    
      <meta name="twitter:url" content="http://localhost:4000/blog/wiki-n-gram-lm/">
    
    
      <meta name="twitter:description" content="My thoughts about the world">
    
    
      <meta name="twitter:image:src" content="http://localhost:4000/assets/img/header_image.jpg">
    


    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Dani's Braindump" href="http://localhost:4000/feed.xml" />

    <!-- Bootstrap: include before theme CSS so styles are overridden -->
    <!--<link rel="stylesheet" href="/assets/css/bootstrap.css">-->

    <!-- FontAwesome icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- custom styles -->
    <link rel="stylesheet" href="/assets/css/custom.css">

    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://localhost:4000/blog/wiki-n-gram-lm/">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>

    <!-- Google Analytics -->

    <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-56931568-3', 'auto');
ga('send', 'pageview');

</script>



<!-- jQuery -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>

<!-- Bootstrap: http://getbootstrap.com/docs/4.1/components/alerts/ -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>


<!-- UniteGallery: http://unitegallery.net/index.php?page=documentation#changing_theme -->
<!-- Final Tiles Gallery: https://www.final-tiles-gallery.com -->
<script src="/assets/js/unitegallery/unitegallery.min.js"></script>
<script src="/assets/js/unitegallery/themes/tiles/ug-theme-tiles.js"></script>
<link rel='stylesheet' href="/assets/css/unitegallery/unite-gallery.css" type='text/css' />
<link rel='stylesheet' href="/assets/js/unitegallery/themes/default/ug-theme-default.css" type='text/css' />

<!-- Unite Gallery-->
<script type="text/javascript">
    $(document).ready(function(){
        $("#gallery").unitegallery({
            // http://unitegallery.net/index.php?page=tiles-justified-options
            gallery_theme: "tiles",
            tiles_type: "justified"
            // ,gallery_width:"75%"
        });
    });
</script>

<!-- MathJax: https://www.mathjax.org/ -->
<!-- Turn on equation numbering: http://docs.mathjax.org/en/latest/tex.html#automatic-equation-numbering -->
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- Hypothes.is (https://web.hypothes.is/) -->

</head>


<body>

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">Dani's Braindump</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Blog</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/ml">Machine Learning</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/contact">Contact</a>
                 </li> 
                 
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post editable">

    <!-- Frenzy layout: dynamically set title image -->
    

    <!-- Overrides frenzy image if set -->
    

    

    <h1 class="postTitle">Creating a n-gram Language Model using Wikipedia</h1>

    <!-- adds a TDLR alert -->
    
        <div class="alert alert-primary" role="alert">
            <strong>TLDR</strong>: This post describes how to train a n-gram Language Model of any order using Wikipedia articles. The code used is available from <a href="https://github.com/tiefenauer/wiki-lm" target="_blank">my GitHub repo</a>.
        </div>
    

    <p class="meta">September 27, 2018 | <span class="time">25</span> Minute Read</p>

    <h4 class="no_toc" id="table-of-contents">Table of Contents</h4>

<ul id="markdown-toc">
  <li><a href="#on-the-road-to-asr" id="markdown-toc-on-the-road-to-asr">On the road to ASR</a></li>
  <li><a href="#creating-a-lm-from-german-wikipedia-articles" id="markdown-toc-creating-a-lm-from-german-wikipedia-articles">Creating a LM from German Wikipedia articles</a>    <ul>
      <li><a href="#preprocessing-the-wikipedia-dump" id="markdown-toc-preprocessing-the-wikipedia-dump">Preprocessing the Wikipedia dump</a></li>
      <li><a href="#building-the-corpus" id="markdown-toc-building-the-corpus">Building the corpus</a></li>
      <li><a href="#training-the-lm" id="markdown-toc-training-the-lm">Training the LM</a></li>
    </ul>
  </li>
  <li><a href="#evaluating-the-lm" id="markdown-toc-evaluating-the-lm">Evaluating the LM</a></li>
  <li><a href="#using-a-lm-to-build-a-simple-spell-checker" id="markdown-toc-using-a-lm-to-build-a-simple-spell-checker">Using a LM to build a simple spell checker</a></li>
  <li><a href="#results--conclusion" id="markdown-toc-results--conclusion">Results &amp; Conclusion</a></li>
</ul>

<p>For my master thesis at <a href="https://www.fhnw.ch/">FHNW</a> I am building a pipeline for <a href="http://www.voxforge.org/home/docs/faq/faq/what-is-forced-alignment">Forced Alignment</a> (FA). This pipeline requires an <a href="https://en.wikipedia.org/wiki/Speech_recognition">Automatic Speech Recognition</a> (ASR) system in one stage to produce partial transcripts for voiced audio segments that were detected using <a href="https://en.wikipedia.org/wiki/Voice_activity_detection">Voice Activity Detection</a> (VAD) from <a href="https://webrtc.org">WebRTC</a>. Luckily, there is a Python module called <a href="https://github.com/wiseman/py-webrtcvad">webrtcvad</a> containing the C-bindings for the VAD-part of WebRTC and is therefore very fast and accurate. Those partial transcripts are then locally aligned with the (known) full transcript using the <a href="https://en.wikipedia.org/wiki/Smith%E2%80%93Waterman_algorithm">Smith Waterman</a> (SM) algorithm (see <a href="/blog/smith-waterman/">my blog post</a> for an implementation in Python).</p>

<h2 id="on-the-road-to-asr">On the road to ASR</h2>

<p>A Speech-to-Text (STT) engine is used to implement the ASR stage. Because of time constraints, I just plugged in an API call to <a href="https://cloud.google.com/speech-to-text/">Google Cloud Speech-to-Text</a> engine and used whatever transcript was returned. This worked reasonably well, although even the STT engine from Google was not error free. Using this API I was able to prove the pipeline approch to be generally working. The downside were the costs that were billed by the minutes of audio transcribed and that I was not able to tune the engine to my needs. Concretely I suspected that a STT engine would suffice, that is not able to recognize speech with the same quality like the Google STT, but still well enough to be useful for the pipeline. So I had to try and train my own STT engine, for which I used a <a href="https://keras.io">Keras</a> implementation of a simplified version of the model presented in the <a href="https://arxiv.org/abs/1412.5567">Deep Speech paper</a>. Note that there is already an <a href="https://github.com/mozilla/DeepSpeech">Mozilla implementation</a> of this model for TensorFlow, but that model aims for high accuracy in speech recognition, which is not what I wanted - my model did not need to be best in class, but merely <em>good enough</em>.</p>

<p>Although the pipeline has four stages (Preprocessing, VAD, ASR, LSA), the overall quality of the produced alignments will highly depend on the quality of the partial transcriptions and therefore on the LSA stage - meaning it would all come down on my simplified STT engine. The LSA stage is able to handle some errors, but only to a certain degree. This means the STT engine is at the heart of the pipeline and should be able to infer transcripts that are <em>good enough</em> for the downstream local alignment stage. The quality of ASR systems is usually measured with the <a href="https://en.wikipedia.org/wiki/Word_error_rate">Word Error Rate</a> (WER). The WER value of an ASR system can often be improved by using a <a href="https://en.wikipedia.org/wiki/Language_model">Language Model</a> (LM). A LM models the probability of a given sentence. The most widely used types of LM are <a href="https://en.wikipedia.org/wiki/N-gram"><script type="math/tex">n</script>-gram</a> models, where <script type="math/tex">n</script> denotes the order of the model.</p>

<h2 id="creating-a-lm-from-german-wikipedia-articles">Creating a LM from German Wikipedia articles</h2>

<p>Since the ASR stage should also work for German recordings, I needed a LM for German. The only pre-trained <script type="math/tex">n</script>-gram model I found was one from <a href="https://cmusphinx.github.io/wiki/download/">CMUSphinx</a>, which is a 3-gram model. The file <code class="highlighter-rouge">cmusphinx-voxforge-de.lm.gz</code> at the <a href="https://sourceforge.net/projects/cmusphinx/files/Acoustic%20and%20Language%20Models/German/">SourceForge</a> download center contains an <a href="https://cmusphinx.github.io/wiki/arpaformat/">ARPA</a> file which contains the probabilities and backoff weights for the 1-, 2- and 3-grams. However, what I wanted was a 4-gram model. Luckily I found <a href="https://github.com/kpu/kenlm">KenLM</a> could be used to train n-gram LMs of any order. I only needed a corpus which contained one German sentence per line, words delimited by whitespace (as described in the <a href="https://kheafield.com/code/kenlm/estimation/">Corpus Formatting Notes</a>). I decided to train on articles and pages from German Wikipedia, which can be downloaded as a dump from <a href="https://dumps.wikimedia.org/">Wikipedia dumps</a>. The dump is a <code class="highlighter-rouge">*.bz2</code>-compressed XML file containing the articles in Wiki markup, which means some heavy preprocessing was needed in order to arrive at a raw text corpus in the expected input format.</p>

<h3 id="preprocessing-the-wikipedia-dump">Preprocessing the Wikipedia dump</h3>

<p>Creating a corpus is no trivial task, especially if the raw data is contained in XML and the actual raw text contains Wiki markup and other special characters, most (!) of which are not wanted in the corpus. Some Google Research revealed a few options for how to extract the raw text from a Wiki dump:</p>

<ol>
  <li><strong><a href="https://dizzylogic.com/wiki-parser">Wiki Parser</a></strong>: This is a tool written in C++ that seemed to work pretty well. The text extraction is comparably fast (1-2 hours for the whole dump), but the result still needs to be postprocessed by removing special characters and splitting the text into one sentence per line. Unfortunately, this tool only runs on Windows (I’m using Ubuntu) and does not offer functionality over a CLI, which means I cannot include it in a script.</li>
  <li><strong><a href="https://radimrehurek.com/gensim/corpora/wikicorpus.html">WikiCorpus</a></strong>: This is the way to go proposed by a <a href="https://www.kdnuggets.com/2017/11/building-wikipedia-text-corpus-nlp.html">post on KDnuggets</a>. <em>WikiCorpus</em> is a class in the <a href="https://radimrehurek.com/gensim/index.html">Gensim</a> module, which can be used to extract the text from a Wikipedia dump as a stream of word tokens. According to the comments in the code, processing takes up to 8 hours, which is quite slow. Additionally, the tokenization into sentences is lost because each article is converted into a flat list of words. The module offers hooks to plug in a custom tokenization algorithm. However, since this is the core part of extracting the raw text, this probably requires a lot of coding to remove Wiki markup and transform all text into the expected output.</li>
  <li><strong><a href="https://github.com/attardi/wikiextractor">WikiExtractor</a></strong>: This is a standalone Python class that can be used to <em>“clean”</em> a Wikipedia corpus, i.e. extract the text from a database dump. I found that processing the dump with this implementation required approximately 2.5 hours on my personal laptop, which was much shorter than the Gensim implementation.</li>
</ol>

<p>After some experimentation, I found that WikiExtractor last option offered the most bang for the buck for my purpose. Unfortunately, WikiExtractor does not write to a text file directly, but rather split the dump file into a bunch of compressed files with similar size. Each of these files contains a number of articles in the <a href="http://medialab.di.unipi.it/wiki/Document_Format">Document Format</a>. This means, the results of WikiExtractor are just an intermediate product that needs further processing. Luckily, I found the amount required for this justifiable, using a combination of Bash and Python scripts to build a raw text corpus. I was able to script the whole process to build the corpus and train the LM on it and will describe the core steps here. You can find the whole implementation in <a href="https://github.com/tiefenauer/wiki-lm">my GitHub repository</a>.</p>

<h3 id="building-the-corpus">Building the corpus</h3>

<p>The following bash script will define a few variables that will be used in the following scripts. Note that <a href="http://www.ivarch.com/programs/pv.shtml">Pipeline Viewer</a> is required to show a nice progress bar.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">corpus_name</span><span class="o">=</span><span class="s2">"wiki_</span><span class="k">${</span><span class="nv">language</span><span class="k">}</span><span class="s2">"</span>
<span class="nv">lm_basename</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">corpus_name</span><span class="k">}</span><span class="s2">_</span><span class="k">${</span><span class="nv">order</span><span class="k">}</span><span class="s2">_gram"</span>
<span class="nv">tmp_dir</span><span class="o">=</span><span class="s2">"./tmp"</span>  <span class="c"># directory for intermediate artifacts</span>
<span class="nv">lm_dir</span><span class="o">=</span><span class="s2">"./"</span> <span class="c"># directory for trained model</span>

<span class="nv">cleaned_dir</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">corpus_name</span><span class="k">}</span><span class="s2">_clean"</span> <span class="c"># directory for WikiExtractor</span>
<span class="nv">corpus_file</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">corpus_name</span><span class="k">}</span><span class="s2">.txt"</span> <span class="c"># uncompressed corpus</span>
<span class="nv">lm_counts</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">corpus_name</span><span class="k">}</span><span class="s2">.counts"</span> <span class="c"># corpus vocabulary with counts (all words)</span>
<span class="nv">lm_vocab</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">corpus_name</span><span class="k">}</span><span class="s2">.vocab"</span> <span class="c"># corpus vocabulary used for training (most frequent words)</span>
<span class="nv">lm_arpa</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">lm_basename</span><span class="k">}</span><span class="s2">.arpa"</span> <span class="c"># ARPA file</span>

<span class="nv">lm_binary</span><span class="o">=</span><span class="s2">"</span><span class="k">${</span><span class="nv">lm_dir</span><span class="k">}</span><span class="s2">/</span><span class="k">${</span><span class="nv">lm_basename</span><span class="k">}</span><span class="s2">.klm"</span> <span class="c"># KenLM binary file (this is the result of the script)</span>
</code></pre></div></div>

<p>The following bash script will download the German Wikipedia dump (~6GB). This will take some time (at least in my case Wiki servers were quite slow when I downloaded the dump).</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>wget <span class="nt">-O</span> <span class="k">${</span><span class="nv">target_file</span><span class="k">}</span> <span class="k">${</span><span class="nv">download_url</span><span class="k">}</span>
</code></pre></div></div>

<p>The following bash script will use WikiExtractor split the dump into the directory defined by <code class="highlighter-rouge">$cleaned_dir</code> (about 1.6 GB). This took about 3 hours on my machine.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>python3 ./WikiExtractor.py <span class="nt">-c</span> <span class="nt">-b</span> 25M <span class="nt">-o</span> <span class="k">${</span><span class="nv">cleaned_dir</span><span class="k">}</span> <span class="k">${</span><span class="nv">target_file</span><span class="k">}</span>
</code></pre></div></div>

<p>The following bash script will read the content of each compresed file and pipe ie to some <a href="http://www.grymoire.com/Unix/Sed.html"><code class="highlighter-rouge">sed</code></a> commands to remove the <code class="highlighter-rouge">&lt;doc&gt;</code>-tag and all kinds of quotation marks. The number of processed articles is counted by <code class="highlighter-rouge">grep</code>ping the number of occurrences of the <code class="highlighter-rouge">&lt;doc&gt;</code>-tag. The result is the normalized and split into lines of sentences by piping it through a Python script called <code class="highlighter-rouge">create_lm.py</code>. This scripts prints its output to stdout and can be written to <code class="highlighter-rouge">$dewiki_txt</code>. This file can then be compressed to save space.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">result</span><span class="o">=</span><span class="k">$(</span>find <span class="nv">$cleaned_dir</span> <span class="nt">-name</span> <span class="s1">'*bz2'</span> <span class="nt">-exec</span> bzcat <span class="o">{}</span> <span class="se">\+</span> <span class="se">\</span>
        | pv <span class="se">\</span>
        | <span class="nb">tee</span> <span class="o">&gt;(</span>    <span class="nb">sed</span> <span class="s1">'s/&lt;[^&gt;]*&gt;//g'</span> <span class="se">\</span>
                  | <span class="nb">sed</span> <span class="s1">'s|["'</span><span class="se">\'</span><span class="s1">'„“‚‘]||g'</span> <span class="se">\</span>
                  | python3 ./create_corpus.py <span class="k">${</span><span class="nv">language</span><span class="k">}</span> <span class="o">&gt;</span> <span class="k">${</span><span class="nv">corpus_file</span><span class="k">}</span> <span class="se">\</span>
               <span class="k">)</span> <span class="se">\</span>
        | <span class="nb">grep</span> <span class="nt">-e</span> <span class="s2">"&lt;doc"</span> <span class="se">\</span>
        | <span class="nb">wc</span> <span class="nt">-l</span><span class="o">)</span>
<span class="nb">echo</span> <span class="s2">"Processed </span><span class="k">${</span><span class="nv">result</span><span class="k">}</span><span class="s2"> articles and saved raw text in </span><span class="nv">$corpus_file</span><span class="s2">"</span>
bzip2 <span class="k">${</span><span class="nv">corpus_file</span><span class="k">}</span>
</code></pre></div></div>

<p>The <a href="https://github.com/tiefenauer/wiki-lm/blob/master/create_corpus.py"><code class="highlighter-rouge">create_lm.py</code></a> script processes each line by splitting it into sentences using <a href="https://www.nltk.org/">NLTK</a>. Each sentence is split into a list of word-tokens. Each token is then procesed by removing unwanted characters, replacing numbers by the <code class="highlighter-rouge">&lt;num&gt;</code>-token, trimming whitespaces and replacing each character with an ASCII character (if that is possible), a process called unidecoding. Note that umlauts are very common in German and should therefore not be replaced. I had to write a slightly modified version of the <a href="https://pypi.org/project/Unidecode/"><code class="highlighter-rouge">unidecode</code></a> function from the Python module of the same name. The modified version will not replace umlauts, but the rest of the logic is identical.</p>

<p>The processed word-tokens are then concatenated again using a single whitespace and made lowercase. The result is the representation of a sentence that can be used for training a KenLM model and is written to stdout. Since the python script is called in a bash script and its output is written directly to the <code class="highlighter-rouge">$dewiki_txt</code>, <code class="highlighter-rouge">$dewiki_txt</code> will contain the raw text data with one sentence per line.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">import</span> <span class="nn">nltk</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">import</span> <span class="nn">string</span>

<span class="kn">from</span> <span class="nn">sys</span> <span class="kn">import</span> <span class="n">version_info</span>
<span class="kn">from</span> <span class="nn">unidecode</span> <span class="kn">import</span> <span class="n">unidecode</span><span class="p">,</span> <span class="n">_warn_if_not_unicode</span><span class="p">,</span> <span class="n">Cache</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">'__main__'</span><span class="p">:</span>
    <span class="n">LANGUAGES</span> <span class="o">=</span> <span class="p">{</span><span class="s">'de'</span><span class="p">:</span> <span class="s">'german'</span><span class="p">,</span> <span class="s">'en'</span><span class="p">:</span> <span class="s">'english'</span><span class="p">}</span>
    <span class="n">lang</span> <span class="o">=</span> <span class="n">LANGUAGES</span><span class="p">[</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]]</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdin</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">process_line</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="n">lang</span><span class="p">):</span>
            <span class="k">print</span><span class="p">(</span><span class="n">sentence</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">process_line</span><span class="p">(</span><span class="n">line</span><span class="p">,</span> <span class="n">min_words</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s">'german'</span><span class="p">):</span>
    <span class="n">sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">sents</span> <span class="o">=</span> <span class="n">nltk</span><span class="o">.</span><span class="n">sent_tokenize</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="n">language</span><span class="o">=</span><span class="n">language</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">sentence</span> <span class="ow">in</span> <span class="n">sents</span><span class="p">:</span>
        <span class="n">sentence_processed</span> <span class="o">=</span> <span class="n">process_sentence</span><span class="p">(</span><span class="n">sentence</span><span class="p">,</span> <span class="n">min_words</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">sentence_processed</span><span class="p">:</span>
            <span class="n">sentences</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sentence_processed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">sentences</span>


<span class="k">def</span> <span class="nf">process_sentence</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">min_words</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="n">normalize_word</span><span class="p">(</span><span class="n">word</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">nltk</span><span class="o">.</span><span class="n">word_tokenize</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">language</span><span class="o">=</span><span class="s">'german'</span><span class="p">)]</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&gt;=</span> <span class="n">min_words</span><span class="p">:</span>
        <span class="k">return</span> <span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">w</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">words</span> <span class="k">if</span> <span class="n">w</span><span class="p">)</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>  <span class="c1"># prevent multiple spaces
</span>    <span class="k">return</span> <span class="s">''</span>


<span class="k">def</span> <span class="nf">normalize_word</span><span class="p">(</span><span class="n">token</span><span class="p">):</span>
    <span class="n">_token</span> <span class="o">=</span> <span class="n">unidecode_keep_umlauts</span><span class="p">(</span><span class="n">token</span><span class="p">)</span>
    <span class="n">_token</span> <span class="o">=</span> <span class="n">remove_punctuation</span><span class="p">(</span><span class="n">_token</span><span class="p">)</span>  <span class="c1"># remove any special chars
</span>    <span class="n">_token</span> <span class="o">=</span> <span class="n">replace_numeric</span><span class="p">(</span><span class="n">_token</span><span class="p">,</span> <span class="n">by_single_digit</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">_token</span> <span class="o">=</span> <span class="s">'&lt;num&gt;'</span> <span class="k">if</span> <span class="n">_token</span> <span class="o">==</span> <span class="s">'#'</span> <span class="k">else</span> <span class="n">_token</span>  <span class="c1"># if token was a number, replace it with &lt;num&gt; token
</span>    <span class="k">return</span> <span class="n">_token</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">remove_punctuation</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">punctiation_extended</span><span class="o">=</span><span class="n">string</span><span class="o">.</span><span class="n">punctuation</span> <span class="o">+</span> <span class="s">""""„“‚‘"""</span><span class="p">):</span>
    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">c</span> <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">text</span> <span class="k">if</span> <span class="n">c</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">punctiation_extended</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">replace_numeric</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">numeric_pattern</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">'[0-9]+'</span><span class="p">),</span> <span class="n">digit_pattern</span><span class="o">=</span><span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">'[0-9]'</span><span class="p">),</span> <span class="n">repl</span><span class="o">=</span><span class="s">'#'</span><span class="p">,</span>
                    <span class="n">by_single_digit</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">numeric_pattern</span><span class="p">,</span> <span class="n">repl</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span> <span class="k">if</span> <span class="n">by_single_digit</span> <span class="k">else</span> <span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="n">digit_pattern</span><span class="p">,</span> <span class="n">repl</span><span class="p">,</span> <span class="n">text</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">contains_numeric</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="k">return</span> <span class="nb">any</span><span class="p">(</span><span class="n">char</span><span class="o">.</span><span class="n">isdigit</span><span class="p">()</span> <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">unidecode_keep_umlauts</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># modified version from unidecode.unidecode_expect_ascii that does not replace umlauts
</span>    <span class="n">_warn_if_not_unicode</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">bytestring</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'ASCII'</span><span class="p">)</span>
    <span class="k">except</span> <span class="nb">UnicodeEncodeError</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">_unidecode_keep_umlauts</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">version_info</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">&gt;=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">text</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">bytestring</span>


<span class="k">def</span> <span class="nf">_unidecode_keep_umlauts</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># modified version from unidecode._unidecode that keeps umlauts
</span>    <span class="n">retval</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">char</span> <span class="ow">in</span> <span class="n">text</span><span class="p">:</span>
        <span class="n">codepoint</span> <span class="o">=</span> <span class="nb">ord</span><span class="p">(</span><span class="n">char</span><span class="p">)</span>

        <span class="c1"># Basic ASCII, ä/Ä, ö/Ö, ü/Ü
</span>        <span class="k">if</span> <span class="n">codepoint</span> <span class="o">&lt;</span> <span class="mh">0x80</span> <span class="ow">or</span> <span class="n">codepoint</span> <span class="ow">in</span> <span class="p">[</span><span class="mh">0xe4</span><span class="p">,</span> <span class="mh">0xc4</span><span class="p">,</span> <span class="mh">0xf6</span><span class="p">,</span> <span class="mh">0xd6</span><span class="p">,</span> <span class="mh">0xfc</span><span class="p">,</span> <span class="mh">0xdc</span><span class="p">]:</span>
            <span class="n">retval</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">char</span><span class="p">))</span>
            <span class="k">continue</span>

        <span class="k">if</span> <span class="n">codepoint</span> <span class="o">&gt;</span> <span class="mh">0xeffff</span><span class="p">:</span>
            <span class="k">continue</span>  <span class="c1"># Characters in Private Use Area and above are ignored
</span>
        <span class="k">if</span> <span class="mh">0xd800</span> <span class="o">&lt;=</span> <span class="n">codepoint</span> <span class="o">&lt;=</span> <span class="mh">0xdfff</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s">"Surrogate character </span><span class="si">%</span><span class="s">r will be ignored. "</span>
                          <span class="s">"You might be using a narrow Python build."</span> <span class="o">%</span> <span class="p">(</span><span class="n">char</span><span class="p">,),</span>
                          <span class="nb">RuntimeWarning</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>

        <span class="n">section</span> <span class="o">=</span> <span class="n">codepoint</span> <span class="o">&gt;&gt;</span> <span class="mi">8</span>  <span class="c1"># Chop off the last two hex digits
</span>        <span class="n">position</span> <span class="o">=</span> <span class="n">codepoint</span> <span class="o">%</span> <span class="mi">256</span>  <span class="c1"># Last two hex digits
</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">table</span> <span class="o">=</span> <span class="n">Cache</span><span class="p">[</span><span class="n">section</span><span class="p">]</span>
        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">mod</span> <span class="o">=</span> <span class="nb">__import__</span><span class="p">(</span><span class="s">'unidecode.x</span><span class="si">%03</span><span class="s">x'</span> <span class="o">%</span> <span class="p">(</span><span class="n">section</span><span class="p">),</span> <span class="nb">globals</span><span class="p">(),</span> <span class="nb">locals</span><span class="p">(),</span> <span class="p">[</span><span class="s">'data'</span><span class="p">])</span>
            <span class="k">except</span> <span class="nb">ImportError</span><span class="p">:</span>
                <span class="n">Cache</span><span class="p">[</span><span class="n">section</span><span class="p">]</span> <span class="o">=</span> <span class="bp">None</span>
                <span class="k">continue</span>  <span class="c1"># No match: ignore this character and carry on.
</span>
            <span class="n">Cache</span><span class="p">[</span><span class="n">section</span><span class="p">]</span> <span class="o">=</span> <span class="n">table</span> <span class="o">=</span> <span class="n">mod</span><span class="o">.</span><span class="n">data</span>

        <span class="k">if</span> <span class="n">table</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">table</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">position</span><span class="p">:</span>
            <span class="n">retval</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">table</span><span class="p">[</span><span class="n">position</span><span class="p">])</span>

    <span class="k">return</span> <span class="s">''</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">retval</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">check_lm</span><span class="p">(</span><span class="n">lm_path</span><span class="p">,</span> <span class="n">vocab_path</span><span class="p">,</span> <span class="n">sentence</span><span class="p">):</span>
    <span class="kn">import</span> <span class="nn">kenlm</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">kenlm</span><span class="o">.</span><span class="n">LanguageModel</span><span class="p">(</span><span class="n">lm_path</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'loaded {model.order}-gram model from {lm_path}'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'sentence: {sentence}'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'score: {model.score(sentence)}'</span><span class="p">)</span>

    <span class="n">words</span> <span class="o">=</span> <span class="p">[</span><span class="s">'&lt;s&gt;'</span><span class="p">]</span> <span class="o">+</span> <span class="n">sentence</span><span class="o">.</span><span class="n">split</span><span class="p">()</span> <span class="o">+</span> <span class="p">[</span><span class="s">'&lt;/s&gt;'</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">prob</span><span class="p">,</span> <span class="n">length</span><span class="p">,</span> <span class="n">oov</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">full_scores</span><span class="p">(</span><span class="n">sentence</span><span class="p">)):</span>
        <span class="n">two_gram</span> <span class="o">=</span> <span class="s">' '</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">-</span> <span class="n">length</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="mi">2</span><span class="p">])</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'{prob} {length}: {two_gram}'</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">oov</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\t\"</span><span class="s">{words[i+1]}" is an OOV!'</span><span class="p">)</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">word</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">open</span><span class="p">(</span><span class="n">vocab_path</span><span class="p">)</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">line</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">())</span>
    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'loaded vocab with {len(vocab)} unique words'</span><span class="p">)</span>
    <span class="k">print</span><span class="p">()</span>
    <span class="n">word</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Your turn now! Start a sentence by writing a word: (enter nothing to abort)</span><span class="se">\n</span><span class="s">'</span><span class="p">)</span>
    <span class="n">sentence</span> <span class="o">=</span> <span class="s">''</span>
    <span class="n">state_in</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="n">kenlm</span><span class="o">.</span><span class="n">State</span><span class="p">(),</span> <span class="n">kenlm</span><span class="o">.</span><span class="n">State</span><span class="p">()</span>
    <span class="n">total_score</span> <span class="o">=</span> <span class="mf">0.0</span>
    <span class="n">model</span><span class="o">.</span><span class="n">BeginSentenceWrite</span><span class="p">(</span><span class="n">state_in</span><span class="p">)</span>

    <span class="k">while</span> <span class="n">word</span><span class="p">:</span>
        <span class="n">sentence</span> <span class="o">+=</span> <span class="s">' '</span> <span class="o">+</span> <span class="n">word</span>
        <span class="n">sentence</span> <span class="o">=</span> <span class="n">sentence</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'sentence: {sentence}'</span><span class="p">)</span>
        <span class="n">total_score</span> <span class="o">+=</span> <span class="n">model</span><span class="o">.</span><span class="n">BaseScore</span><span class="p">(</span><span class="n">state_in</span><span class="p">,</span> <span class="n">word</span><span class="p">,</span> <span class="n">state_out</span><span class="p">)</span>

        <span class="n">candidates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">((</span><span class="n">model</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">sentence</span> <span class="o">+</span> <span class="s">' '</span> <span class="o">+</span> <span class="n">next_word</span><span class="p">),</span> <span class="n">next_word</span><span class="p">)</span> <span class="k">for</span> <span class="n">next_word</span> <span class="ow">in</span> <span class="n">vocab</span><span class="p">)</span>
        <span class="n">bad_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="n">top_words</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">candidates</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="n">itemgetter</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
        <span class="n">worst_5</span> <span class="o">=</span> <span class="n">bad_words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
        <span class="k">print</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'least probable 5 next words:'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">worst_5</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\t</span><span class="s">{w}</span><span class="se">\t\t</span><span class="s">{s}'</span><span class="p">)</span>

        <span class="n">best_5</span> <span class="o">=</span> <span class="n">top_words</span><span class="p">[:</span><span class="mi">5</span><span class="p">]</span>
        <span class="k">print</span><span class="p">()</span>
        <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'most probable 5 next words:'</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">w</span><span class="p">,</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">best_5</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'</span><span class="se">\t</span><span class="s">{w}</span><span class="se">\t\t</span><span class="s">{s}'</span><span class="p">)</span>

        <span class="k">if</span> <span class="s">'.'</span> <span class="ow">in</span> <span class="n">word</span><span class="p">:</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'score for sentence </span><span class="se">\"</span><span class="s">{sentence}</span><span class="se">\"</span><span class="s">:</span><span class="se">\t</span><span class="s"> {total_score}"'</span><span class="p">)</span>  <span class="c1"># same as model.score(sentence)!
</span>            <span class="n">sentence</span> <span class="o">=</span> <span class="s">''</span>
            <span class="n">state_in</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="n">kenlm</span><span class="o">.</span><span class="n">State</span><span class="p">(),</span> <span class="n">kenlm</span><span class="o">.</span><span class="n">State</span><span class="p">()</span>
            <span class="n">model</span><span class="o">.</span><span class="n">BeginSentenceWrite</span><span class="p">(</span><span class="n">state_in</span><span class="p">)</span>
            <span class="n">total_score</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'Start a new sentence!'</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">state_in</span><span class="p">,</span> <span class="n">state_out</span> <span class="o">=</span> <span class="n">state_out</span><span class="p">,</span> <span class="n">state_in</span>

        <span class="n">word</span> <span class="o">=</span> <span class="nb">input</span><span class="p">(</span><span class="s">'Enter next word: '</span><span class="p">)</span>

    <span class="k">print</span><span class="p">(</span><span class="n">f</span><span class="s">'That</span><span class="se">\'</span><span class="s">s all folks. Thanks for watching.'</span><span class="p">)</span>
</code></pre></div></div>

<p>Following the steps above I arrived at a corpus, which stored the entire German encyclopedia from Wikipedia in a single corpus text file at <code class="highlighter-rouge">$dewiki_txt</code>. The corpus file is approximately 5GB in size (xxxGB compressed) and contains 42,229,452 sentences (712,167,726 words) from ~2.2 million articles. Here’s an excerpt from the <a href="https://de.wikipedia.org/wiki/Spracherkennung#Sprachmodell">German Wikipedia article about Language Models</a>:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>ebenfalls im jahr &lt;num&gt; erschienen zwei bestofalben mit den grössten hits der band best of the braving days und best of the awakening days
am &lt;num&gt; april &lt;num&gt; stellten galneryus ihr erstes studioprojekt mit sho vor die aus drei tracks bestehende ep beginning of the resurrection
bei einem der tracks handelt es sich um das abspannlied a faroff distance der animeserie
am &lt;num&gt; juni wurde das sechste album resurrection veröffentlicht
die unterschiede zwischen yamab und sho sind hier deutlich wahrnehmbar
dennoch ist es der band seitdem gelungen eine stilistischklangliche kontinuität zu bewahren und musikalische elemente einzubauen die an die frühen galneryusalben erinnern
mit phoenix rising veröffentlichte die band am &lt;num&gt; oktober &lt;num&gt; ihr siebtes studio album
im januar &lt;num&gt; veröffentlichte die band ein minialbum unter dem namen kizuna der gleichnamige song wurde in dem pachinko game pachinko cr first of the blue sky verwendet
im juli des gleichen jahres folgte eine neue single hunting for your dream welches als abspannlied der anime serie hunter x hunter veröffentlicht wurde
das lang erwartete achte studio album angel of salvation erschien am &lt;num&gt; oktober &lt;num&gt;
der namensgebende titel angel of salvation war bis zu diesem zeitpunkt der längste song in der bandgeschichte

</code></pre></div></div>

<h3 id="training-the-lm">Training the LM</h3>

<p>Training the KenLM model requires building the project using cmake and other tools (see the <a href="https://kheafield.com/code/kenlm/">KenLM documentation</a> for more details), which only works on Unix based systems. Make sure, you have the resulting <code class="highlighter-rouge">bin</code> folder on the path to use <code class="highlighter-rouge">lmplz</code> and <code class="highlighter-rouge">build_binary</code>. Also, make sure the temporary directory set with <code class="highlighter-rouge">-T</code> provides enough free storage, otherwise training will fail with a message like <code class="highlighter-rouge">Last element should be poison</code>.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Training </span><span class="nv">$N</span><span class="s2">-gram KenLM model with data from </span><span class="nv">$dewiki_txt</span><span class="s2"> and saving ARPA file to </span><span class="nv">$lm_arpa</span><span class="s2">"</span>
lmplz <span class="nt">-o</span> <span class="k">${</span><span class="nv">order</span><span class="k">}</span> <span class="nt">-T</span> <span class="k">${</span><span class="nv">tmp_dir</span><span class="k">}</span> <span class="nt">-S</span> 40% <span class="nt">--limit_vocab_file</span> <span class="k">${</span><span class="nv">lm_vocab</span><span class="k">}</span> &lt;<span class="k">${</span><span class="nv">corpus_file</span><span class="k">}</span>.bz2
</code></pre></div></div>

<p>After building the ARPA file, this file can be converte to a binary file, which loads faster. Note that KenLM works with any ARPA files, so you could even convert the ARPA file from CMUSphinx mentioned above.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"Building binary file from </span><span class="nv">$lm_arpa</span><span class="s2"> and saving to </span><span class="nv">$lm_binary</span><span class="s2">"</span>
build_binary trie <span class="k">${</span><span class="nv">lm_arpa</span><span class="k">}</span> <span class="k">${</span><span class="nv">lm_binary</span><span class="k">}</span>
</code></pre></div></div>

<p>Running all the steps through here will produce an ARPA file at <code class="highlighter-rouge">$lm_arpa</code> and a binary KenLM model in <code class="highlighter-rouge">lm_binary</code>. If a sorted vocabulary of all the unique words is require, this can be obtained by running the following command. Note that the vocabulary is extracted from the corpus the LM was trained on, this works only for unpruned models.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">echo</span> <span class="s2">"(re-)creating vocabulary of </span><span class="nv">$dewiki_txt</span><span class="s2"> and saving it in </span><span class="nv">$lm_vocab</span><span class="s2">"</span>
<span class="nb">grep</span> <span class="nt">-oE</span> <span class="s1">'\w+'</span> <span class="nv">$dewiki_txt</span> | pv <span class="nt">-s</span> <span class="k">$(</span><span class="nb">stat</span> <span class="nt">--printf</span><span class="o">=</span><span class="s2">"%s"</span> <span class="nv">$dewiki_txt</span><span class="k">)</span> | <span class="nb">sort</span> <span class="nt">-u</span> <span class="nt">-f</span> <span class="o">&gt;</span> <span class="nv">$lm_vocab</span>
</code></pre></div></div>

<h2 id="evaluating-the-lm">Evaluating the LM</h2>
<p>The raw text corpus contains more than 700 million words from 42 million sentences in 2.2 million articles. The vocabulary size (i.e. the number of unique words) is about 8.3 million. I used it to train a 2-gram and a 4-gram KenLM model using my personal Laptop using an i7 processor with 4 cores, 8GB RAM and an SSD hard disk. Creating the raw text corpus from the Wikipedia dump took the most time. After that, it was more or less smooth sailing, apart from some fiddling with the <code class="highlighter-rouge">lmplz</code> parameters <code class="highlighter-rouge">-T</code> (to make sure to use a temporary directory with enough space) and and <code class="highlighter-rouge">-S</code> (to make sure not to use too much memory). The final model uses about 2.3G (2-gram) resp. 18G (4-gram) of disk space.</p>

<p>According to Dan Jurafsky bible <em><a href="https://web.stanford.edu/~jurafsky/slp3/">Speech and Language Processing</a>_The best way to evaluate a n-gram LM is to embed it in an application and measure how much the application improves (called _extrinsic evaluation</em>). <em>Intrinsic evaluation</em> describes measuring the performance of a LM independent from any application and would require scoring sentences on a training set, which were never seen before. The results can then be compared to a reference LM: Whatever model produces higher probabilities (or lower perplexity) to the <script type="math/tex">n</script>-grams in the test set is deemed to perform better.</p>

<p>Because of time constraints and because KenLM has already been extensively evaluated on English I refrained from evaluating my German LM intrinsically, although the corpus used for training is not as big as the one used by Ken Heafield. To still get an intuition about how well the model performs, the model’s score on some test sentences were calculated. To make sure the sentences could not have been seen during training, the following set of 5 sentences of the current newspaper (a date after creation of the Wikipedia dump) was used:</p>

<h2 id="using-a-lm-to-build-a-simple-spell-checker">Using a LM to build a simple spell checker</h2>

<p>With my newly built LM I was now able to improve the quality of the transcripts produced by my simplified STT engine by using it as a rudimentary spell checker. The LM will post-process the transcripts by going through it word by word and create a list of possible spellings for each word. To do this, we check if the word is in the vocabulary of the LM. If it is, we can assume the word was correctly inferred i.e. it does not only sound right but is also correctly spelled. If it is not in the vocabulary of the LM, we create a list of words with <a href="https://en.wikipedia.org/wiki/Edit_distance">edit distance</a> (<script type="math/tex">ed</script>) 1. If none of these words are in the vocabulary of the LM, create a list of words with <script type="math/tex">ed(w)=2</script> (can be recursively done from the list of words with <script type="math/tex">ed(w)=1</script>). If none of the words from this list are in the vocabulary of the LM, keep the original word and accept that it might have been incorrectly transcribed (i.e. with an <script type="math/tex">ed(w)>1</script>), the word is completely wrong (i.e. e.g. <em>their</em> instead of <em>they’re</em>) or the word has simply never been seen while training the LM.</p>

<p>Concatenating the lists of possible spellings gives us a matrix of words. A LM can now assess the probability of each path by calculating the probability of the sentence that is created by concatenating all the words from a path. The sentence can then be corrected by taking the most probable sentence. Note, that the number of paths can become exponentially huge when proceeding as described, requiring dynamic programming and merging paths to calculate the most probable path. I implemented a greedy variant which only keeps the 1.024 most probable sequences after each step (i.e. after adding the list of possible spellings for a word).</p>

<h2 id="results--conclusion">Results &amp; Conclusion</h2>



    <!-- POST NAVIGATION -->
      <div class="postNav clearfix">
     
      <a class="prev" href="/blog/smith-waterman/"><span>&laquo;&nbsp;Smith-Waterman algorithm in Python</span>
      
    </a>
      
      
      <a class="next" href="/blog/stt-engine-for-forced-alignment/"><span>STT-Engine for Forced Alignment&nbsp;&raquo;</span>
       
      </a>
     
    
    
    <p id="disqus_thread"></p>
    <script>

        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/
        var disqus_config = function () {
            this.page.url = "https://tiefenauer.github.io";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.title = "Creating a n-gram Language Model using Wikipedia"
            this.page.identifier = "/blog/wiki-n-gram-lm/"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://tiefenauer.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>

  </div>
  
</div>

      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2019 <a href="http://www.tiefenauer.info">Daniel Tiefenauer</a>
      <!-- Powered by <a href="http://jekyllrb.com">Jekyll</a> with adapted <a href="https://github.com/brianmaierjr/long-haul">Long Haul</a> Theme -->
      </p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/daniel.tiefenauer">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://twitter.com/danitiefenauer">
                  <svg id="twitter" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/tiefenauer">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="https://www.linkedin.com/in/danieltiefenauer">
                <svg id="linkedin" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                </svg>
            </a></li>
             
            
            <li><a href="https://www.xing.com/profile/Daniel_Tiefenauer/cv">
                <svg id="xing" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M14.887 24l-5.324-9.667 8.07-14.333h4.933l-8.069 14.333 5.27 9.667h-4.88zm-7.291-19h-4.939l2.768 4.744-4.115 7.256h4.914l4.117-7.271-2.745-4.729z"/>
                </svg>
            </a></li>
                         
            
            <li><a href="mailto:daniel@tiefenauer.info">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->


   <!-- Bootstrap scripts-->
<script>
    $('.alert').alert()
</script>

</body>

</html>
