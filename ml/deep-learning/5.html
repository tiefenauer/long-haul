<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]--><!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]--><!--[if IE 8]><html class="no-js lt-ie9" <![endif]--><!--[if gt IE 8]><!--><html class="no-js"> <!--<![endif]-->

<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>Deep Learning (5/5): Sequence Models</title>

    <!-- Open Graph Meta -->
    <meta content="Dani's Braindump" property="og:site_name">
    
      <meta content="Deep Learning (5/5): Sequence Models" property="og:title">
    
    
      <meta content="article" property="og:type">
    
    
      <meta content="My thoughts about the world" property="og:description">
    
    
      <meta content="http://localhost:4000/ml/deep-learning/5" property="og:url">
    
    
    
      <meta content="http://localhost:4000/assets/img/touring.jpg" property="og:image">
    

    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@danitiefenauer">
    <meta name="twitter:creator" content="@danitiefenauer">
    
      <meta name="twitter:title" content="Deep Learning (5/5): Sequence Models">
    
    
      <meta name="twitter:url" content="http://localhost:4000/ml/deep-learning/5">
    
    
      <meta name="twitter:description" content="My thoughts about the world">
    
    
      <meta name="twitter:image:src" content="http://localhost:4000/assets/img/touring.jpg">
    


    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico">

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Dani's Braindump" href="http://localhost:4000/feed.xml">

    <!-- Bootstrap: include before theme CSS so styles are overridden -->
    <!--<link rel="stylesheet" href="/assets/css/bootstrap.css">-->

    <!-- FontAwesome icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- custom styles -->
    <link rel="stylesheet" href="/assets/css/custom.css">

    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="http://localhost:4000/ml/deep-learning/5">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>



     <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-56931568-3', 'auto');
ga('send', 'pageview');

</script>
 
</head>


<body>

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">Dani's Braindump</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Home</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/about">About</a>
                 </li> 
                 
                 <li class="element  current ">
                     <a href="/ml">Machine Learning</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/switzerland">Swiss Frenzy</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/contact">Contact</a>
                 </li> 
                 
             </ul>
         </nav>
     </div>
 </div>
<!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
    

    <h1 class="postTitle">Deep Learning (5/5): Sequence Models</h1>

    
        <p class="meta"><span class="time">52</span> Minute Read</p>
    

    
        <p class="alert alert-primary alert-dismissible fade show" role="alert">
    This page uses <strong><a href="https://web.hypothes.is/" target="_blank">Hypothes.is</a></strong>. You can annotate or highlight text directly on this page by expanding the bar on the right. If you find any errors, typos or you think some explanation is not clear enough, please feel free to add a comment. This helps me improving the quality of this site. <strong>Thank you!</strong>
    <button type="button" class="close" data-dismiss="alert" aria-label="Close">
        <span aria-hidden="true">×</span>
    </button>
</p>
    

    
        <p style="text-align:center">

    <span class="badge badge-secondary">Recurrent Neural Networks (RNN)</span>

    <span class="badge badge-secondary">Sequence Tokens</span>

    <span class="badge badge-secondary">Many-to-Many</span>

    <span class="badge badge-secondary">Many-to-One</span>

    <span class="badge badge-secondary">One-To-Many</span>

    <span class="badge badge-secondary">Gradient Clipping</span>

    <span class="badge badge-secondary">Gated Recurrent Unit (GRU)</span>

    <span class="badge badge-secondary">Long Short Term Unit (LSTM)</span>

    <span class="badge badge-secondary">Peephole Connection</span>

    <span class="badge badge-secondary">Bidirectional RNN (BRNN)</span>

    <span class="badge badge-secondary">Deep-RNN</span>

    <span class="badge badge-secondary">Word Embeddings</span>

    <span class="badge badge-secondary">T-SNE</span>

    <span class="badge badge-secondary">Word2Vec &amp; GloVe</span>

    <span class="badge badge-secondary">Cosinus-Similarity</span>

    <span class="badge badge-secondary">One-Hot-Encoding</span>

    <span class="badge badge-secondary">Skip-Gram</span>

    <span class="badge badge-secondary">CBOW</span>

    <span class="badge badge-secondary">Negative Sampling</span>

    <span class="badge badge-secondary">Context &amp; Target-Word</span>

    <span class="badge badge-secondary">Sentiment Classification</span>

    <span class="badge badge-secondary">Debiasing</span>

    <span class="badge badge-secondary">Sequence-to-Sequence Models</span>

    <span class="badge badge-secondary">Encoder/Decoder-Networks</span>

    <span class="badge badge-secondary">Conditional Language Models</span>

    <span class="badge badge-secondary">Attention Models</span>

    <span class="badge badge-secondary">Beam Search</span>

    <span class="badge badge-secondary">Length Normalization</span>

    <span class="badge badge-secondary">Bleu Score</span>

    <span class="badge badge-secondary">Connectionist Temoral Classification (CTC)</span>

    <span class="badge badge-secondary">Trigger Word Detection</span>

</p>
    

    
        <p class="intro">
    <span class="dropcap">S</span>
    </p>
<p>equence models are a special form of neural networks that take their input as a sequence of tokens. They are often applied in ML tasks such as speech recognition, Natural Language Processing or bioinformatics (like processing DNA sequences).</p>


    

    <div>
<h4 class="no_toc" id="table-of-contents">Table of Contents</h4>

<ul id="markdown-toc">
  <li><a href="#coursera-course-overview" id="markdown-toc-coursera-course-overview">Coursera Course overview</a></li>
  <li><a href="#other-resources" id="markdown-toc-other-resources">Other resources</a></li>
  <li><a href="#sequence-models" id="markdown-toc-sequence-models">Sequence models</a></li>
  <li>
<a href="#recurrent-neural-networks" id="markdown-toc-recurrent-neural-networks">Recurrent Neural Networks</a>    <ul>
      <li><a href="#unidirectional-rnn" id="markdown-toc-unidirectional-rnn">Unidirectional RNN</a></li>
      <li><a href="#forward-propagation" id="markdown-toc-forward-propagation">Forward propagation</a></li>
      <li><a href="#backpropagation" id="markdown-toc-backpropagation">Backpropagation</a></li>
      <li>
<a href="#rnn-architectures" id="markdown-toc-rnn-architectures">RNN architectures</a>        <ul>
          <li><a href="#encoder-decoder-networks" id="markdown-toc-encoder-decoder-networks">Encoder-Decoder Networks</a></li>
        </ul>
      </li>
      <li><a href="#language-model-and-sequence-generation" id="markdown-toc-language-model-and-sequence-generation">Language model and sequence generation</a></li>
      <li>
<a href="#vanishing-gradients-in-rnn" id="markdown-toc-vanishing-gradients-in-rnn">Vanishing Gradients in RNN</a>        <ul>
          <li>
<a href="#gated-recurrent-units" id="markdown-toc-gated-recurrent-units">Gated Recurrent Units</a>            <ul>
              <li><a href="#simple-gru" id="markdown-toc-simple-gru">Simple GRU</a></li>
              <li><a href="#full-gru" id="markdown-toc-full-gru">Full GRU</a></li>
            </ul>
          </li>
          <li><a href="#long-short-term-memory-units" id="markdown-toc-long-short-term-memory-units">Long Short Term Memory Units</a></li>
          <li><a href="#gru-vs-lstm" id="markdown-toc-gru-vs-lstm">GRU vs. LSTM</a></li>
        </ul>
      </li>
      <li><a href="#bidirectional-rnns" id="markdown-toc-bidirectional-rnns">Bidirectional RNNs</a></li>
      <li><a href="#deep-rnn" id="markdown-toc-deep-rnn">Deep RNN</a></li>
    </ul>
  </li>
  <li>
<a href="#natural-language-processing" id="markdown-toc-natural-language-processing">Natural Language Processing</a>    <ul>
      <li><a href="#properties-of-word-embeddings" id="markdown-toc-properties-of-word-embeddings">Properties of word embeddings</a></li>
      <li><a href="#embedding-matrix" id="markdown-toc-embedding-matrix">Embedding matrix</a></li>
      <li>
<a href="#word2vec" id="markdown-toc-word2vec">Word2Vec</a>        <ul>
          <li><a href="#skip-gram" id="markdown-toc-skip-gram">Skip-Gram</a></li>
          <li><a href="#negative-sampling" id="markdown-toc-negative-sampling">Negative Sampling</a></li>
        </ul>
      </li>
      <li><a href="#glove" id="markdown-toc-glove">GloVe</a></li>
      <li><a href="#sentiment-classification" id="markdown-toc-sentiment-classification">Sentiment classification</a></li>
      <li><a href="#debiasing-word-embeddings" id="markdown-toc-debiasing-word-embeddings">Debiasing Word Embeddings</a></li>
    </ul>
  </li>
  <li>
<a href="#sequence-to-sequence-models" id="markdown-toc-sequence-to-sequence-models">Sequence-to-sequence models</a>    <ul>
      <li><a href="#s2s-in-machine-translation" id="markdown-toc-s2s-in-machine-translation">S2S in machine translation</a></li>
      <li>
<a href="#beam-search" id="markdown-toc-beam-search">Beam search</a>        <ul>
          <li>
<a href="#refinements-to-beam-search" id="markdown-toc-refinements-to-beam-search">Refinements to Beam Search</a>            <ul>
              <li><a href="#length-normalization" id="markdown-toc-length-normalization">Length normalization</a></li>
              <li><a href="#error-analysis" id="markdown-toc-error-analysis">Error Analysis</a></li>
            </ul>
          </li>
          <li><a href="#bleu-score" id="markdown-toc-bleu-score">Bleu Score</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#attention-models" id="markdown-toc-attention-models">Attention models</a></li>
  <li>
<a href="#speech-recognition" id="markdown-toc-speech-recognition">Speech recognition</a>    <ul>
      <li><a href="#trigger-word-detection" id="markdown-toc-trigger-word-detection">Trigger word detection</a></li>
    </ul>
  </li>
</ul>

<h2 id="coursera-course-overview">Coursera Course overview</h2>
<p>In the <strong>first week</strong> you know Recurrent Neural Networks (RNN) as a special form of NN and what types of problems they’re good at. You also learn why a traditional NN is not suitable for these kinds of problems. In the first week’s assignment you will implement two generative models: a RNN that can generate music that sounds like a Jazz improvisation. You also implement another form of an RNN that uses textual input to generate random names for dinosaurs.
The <strong>second week</strong> is all about Natural Language Processing (NLP). You learn how word embeddings can help you with NLP tasks and how you can deal with bias. In the second week you will implement some core functions of NLP models such as calculating the similarity between two words or removing the gender bias. You will also implement a RNN that can classify an arbitrary text with a suitable Emoji.
The last and <strong>final week</strong> of this specialization introduces the concept of Attention Models as a special form of Sequence-to-Sequence models and how they can be used for machine translation. You will put your newly learned knowledge about Attention Models into practice by implementing some functions of an RNN that can be used for machine translation. You will also learn how to implement a model that is able to detect trigger words from audio clips.</p>

<h2 id="other-resources">Other resources</h2>

<table>
  <thead>
    <tr>
      <th>Description</th>
      <th>Link</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Adam Coates giving a lecture about speech recognition. Some topics of this page are covered. If you’re not in the mood for reading, watch this! Fun fact: at <code class="highlighter-rouge">0:13</code> you can see Andrew Ng sneak in <img class="emoji" title=":smile:" alt=":smile:" src="https://assets-cdn.github.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">
</td>
      <td><a href="https://www.youtube.com/watch?v=g-sndkf7mCs">YouTube</a></td>
    </tr>
    <tr>
      <td>
<em><strong>What does it really mean for a robot to be racist?</strong></em>: Why bias can become a problem in language models</td>
      <td><a href="https://thegradient.pub/ai-bias/">thegradient.pub</a></td>
    </tr>
    <tr>
      <td>Why bias matters and how to mitigate it.</td>
      <td><a href="https://developers.googleblog.com/2018/04/text-embedding-models-contain-bias.html">Google Developers Blog</a></td>
    </tr>
    <tr>
      <td>
<em><strong>The Unreasonable Effectiveness of Recurrent Neural Networks</strong></em>: Why RNNs are cool</td>
      <td><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">karpathy.github.io</a></td>
    </tr>
  </tbody>
</table>

<h2 id="sequence-models">Sequence models</h2>
<p>The previously seen models processed some sort of input (e.g. images) which exhibited following properties:</p>
<ul>
  <li>it was uniform (e.g. an image of a certain size)</li>
  <li>it was processed as a whole (i.e. an image was not partially processed)</li>
  <li>it was often multidimensional (e.g. an image with 3 color channels yields a matrix of <script type="math/tex">B \times H \times 3</script>)</li>
</ul>

<p>Sequence models are a bit different in that they require their input to be a sequence of tokens. Examples for such sequences could be:</p>

<ul>
  <li>audio data (sequence of sounds)</li>
  <li>text (sequence of words)</li>
  <li>video (sequence of images)</li>
  <li>…</li>
</ul>

<p>The length of the individual input elements (i.e. their number of tokens) does not need to be of the same length, neither for training nor prediction. These input tokens are processed one after the other, whereas at each time step a certain token is processed. Processing can be stopped at any point. A form of sequence models are <strong>Recurrent Neural Networks (RNN)</strong> which are often used to process speech data (e.g. speech recognition, machine translation), generative models (e.g. generating music) or NLP (e.g. sentiment analysis, named entity recognition (NER), …).</p>

<p>The notation for an input sequence <script type="math/tex">x</script> of length <script type="math/tex">T_x</script> or an output sequence <script type="math/tex">y</script> of length <script type="math/tex">T_y</script> is as follows (note the new notation with chevrons around the indices to enumerate the tokens):</p>

<script type="math/tex; mode=display">% <![CDATA[
x = x^{<1>}, x^{<2>}, ..., x^{<t>}, ..., x^{<T_x>} \\
y = y^{<1>}, y^{<2>}, ..., y^{<t>}, ..., y^{<T_y>} %]]></script>

<p>As stated above, the input and the output sequence don’t need to be of the same length (<script type="math/tex">T_x^{(i)} \neq T_y^{(i)}</script>). Also the length of the individual training samples can vary (<script type="math/tex">T_y^{(i)} \neq T_y^{(j)}</script>).</p>

<h2 id="recurrent-neural-networks">Recurrent Neural Networks</h2>

<p>The previously seen approach of a NN with an input layer, several hidden layers and an output layer is not feasible for the following reasons:</p>

<ul>
  <li>input and output can have different lengths for each sample (e.g. sentences with different numbers of words)</li>
  <li>the samples don’t share common features (e.g. in NER, where the named entity can be at any position in the sentence)</li>
</ul>

<p>Because RNN process their input token by token they don’t suffer from these disadvantages. A simple RNN only has one layer through which the tokens pass during training/processing. However, the result of this processing has an influence on the processing of the next token. Consider the following sample architecture of a simple RNN:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/rnn.png" alt="Example of an RNN">
	<figcaption>Example of an RNN (Credits: Coursera)</figcaption>
</figure>

<p>A side effect of this kind of processing is that an RNN requires far less parameters to be optimized than e.g. a ConvNet would to do the same task. This especially comes in handy for sentence processing where each word (token) can be a vector of dimension e.g. <script type="math/tex">10'000 \times 1</script>.</p>

<h4 id="unidirectional-rnn">Unidirectional RNN</h4>

<p>As seen in the picture above the RNN processes each token <script type="math/tex">% <![CDATA[
x^{<t>} %]]></script> individually from left to right, one after the other. In each step <script type="math/tex">t</script> the RNN tries to predict the output <script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script> from the input token <script type="math/tex">% <![CDATA[
x^{<t>} %]]></script> and the previous activation <script type="math/tex">% <![CDATA[
a^{<t-1>} %]]></script>. To determine the influence of the activation and the input token and the two weight matrices <script type="math/tex">W_{aa}</script> and <script type="math/tex">W_{ax}</script> are used. There is also a matrix <script type="math/tex">W_{ya}</script> that governs the output predictions. Those matrices are the same for each step, i.e. they are shared for a single training instance. This way the layer is recursively used to process the sequence. A single input token can therefore not only directly influence the output at a given time step, but also indirectly the output of subsequent steps (thus the term <em>recurrent</em>). Vice versa a single prediction at time step <script type="math/tex">% <![CDATA[
<t> %]]></script> not only depends on a single input token, but on several previously seen tokens (we will see how to expand this so that also following tokens are taken into consideration in <a href="#bi-directional-rnns">bidirectional RNNs</a>).</p>

<h4 id="forward-propagation">Forward propagation</h4>

<p>The activation <script type="math/tex">% <![CDATA[
a^{<t>} %]]></script> and prediction <script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script> for a single time step <script type="math/tex">t</script> can be calculated as follows (for the first token the zero vector is often used as the previous activation):</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
a^{<t>} = g_1(W_{aa} a^{<t-1>} + W_{ax} x^{<t>} + b_a ) \\
\hat{y}^{<t>} = g_2( W_ya a^{<t>} + b_y )
\label{forward_prop}
\end{equation} %]]></script>

<p>Note that the activation functions <script type="math/tex">g_1</script> and <script type="math/tex">g_2</script> can be different. The activation function to calaculate the next activation (<script type="math/tex">g_1</script>) is often Tanh or ReLU. The activation function to predict the next output (<script type="math/tex">g_2</script>) is often the Sigmoid function for binary classification or else Softmax. The notation of the weight matrices is by convention as thas the first index denotes the output quantity and the second index the input quantity. <script type="math/tex">W_{ax}</script> for example means “<em>use the weights in <script type="math/tex">W</script> to compute some output <script type="math/tex">a</script> from input <script type="math/tex">x</script></em>”.</p>

<p>This calculation can further be simplified by concatenating the matrices <script type="math/tex">W_aa</script> and <script type="math/tex">W_ax</script> into a single matrix <script type="math/tex">W_a</script> and stacking:</p>

<script type="math/tex; mode=display">% <![CDATA[
W_a = \left[ W_{aa} \vert W_{ax} \right] \\
\left[ a^{<t-1>}, x^{<t>} \right] =
\begin{bmatrix}
a^{<t-1>} \\
x^{<t>}
\end{bmatrix} %]]></script>

<p>The simplified formula to calculate forward propagation is then:</p>

<script type="math/tex; mode=display">% <![CDATA[
a^{<t>} = g_1(W_a \left[ a^{<t-1>}, x^{<t>} \right] + b_a ) \\
\hat{y}^{<t>} = g_2( W_y a^{<t>} + b_y ) %]]></script>

<p>Note that the formula to calculate <script type="math/tex">\hat{y}</script> only changed in the subscripts used for the weight matrix. This simplified notation is equivalent to <script type="math/tex">\ref{forward_prop}</script> but only uses one weight matrix instead of two.</p>

<h4 id="backpropagation">Backpropagation</h4>
<p>Because the input is read sequentially and the RNN computes a prediction in each step, the output is a sequence of predictions. The loss function for backprop for a single time step <script type="math/tex">% <![CDATA[
{<t>} %]]></script> could be:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\mathcal{L}^{<t>} (\hat{y}^{<t>}, y^{<t>}) = -y^{<t>} \log{\hat{y}^{<t>}} - (1 - y^{<t>}) \log(1-\hat{y}^{<t>})
\label{loss}
\end{equation} %]]></script>

<p>The formula to compute the overall cost for a sequence of <script type="math/tex">T_x</script> predictions is therefore:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\mathcal{L} (\hat{y}, y) = \sum_{t=1}^{T_y} \mathcal{L}^{<t>} (\hat{y}^{<t>}, y^{<t>})
\label{cost}
\end{equation} %]]></script>

<h4 id="rnn-architectures">RNN architectures</h4>

<p>There are different types of network architectures for RNN in terms of how the length of the input relates to the length of the output. A RNN can take a sequence of several tokens as an input and only produce a single value as an output. Such an architecture is called <strong>many-to-one</strong> and is used for tasks like sentiment analysis where the RNN e.g. tries to predict a movie rating based on a textual description of the critics.</p>

<p><img src="/assets/img/articles/ml/dl_5/many-to-one.png" alt="Many-to-one architecture"></p>

<p>The opposite is also possible: A RNN can take only a single value as input and produce a sequence as an output by re-using the previous outputs to make the next prediction. Such an architecture is called <strong>one-to-many</strong>. It could be used for example in a RNN that generates music by taking a genre as an input and generates a sequence of notes as an output.</p>

<p><img src="/assets/img/articles/ml/dl_5/one-to-many.png" alt="One-to-many architecture"></p>

<p>There is theoretically also a <strong>one-to-one</strong> architecture. However, such an architecture is rarely encountered since it essentially corresponds to a standard NN.</p>

<p>Finally, there are networks which take an input sequence of length <script type="math/tex">T_x</script> and produce an output of length <script type="math/tex">T_y</script>. This is called a <strong>many-to-many</strong> architecture. In the above example, the length of the input was equal to the length of the output. However, input and output sequences need not to be of the same length. This property is especially important for tasks like machine translation where the translated text might be longer or shorter than the original text.</p>

<p><img src="/assets/img/articles/ml/dl_5/many-to-many.png" alt="Many-to-many architecture"></p>

<h5 id="encoder-decoder-networks">Encoder-Decoder Networks</h5>
<p>Models with a many-to-one architecture might be implemented as <strong>encoder-decoder</strong> models. This is perhaps the most commonly used framework for sequence modelling with neural networks. Like the name suggests, an Encoder-Decoder model consists of two RNNs. The <strong>encoder</strong> maps the input sequence <script type="math/tex">X</script> to a hidden representation <script type="math/tex">H</script> of the same length as the input. The <strong>decoder</strong> then consumes this hidden representation to produce <script type="math/tex">Y</script>, i.e. make a prediction.</p>

<script type="math/tex; mode=display">H = encode(X) \\
Y=p(Y \vert X) = decode H</script>

<p><img src="/assets/img/articles/ml/dl_5/encoder-decoder.png" alt="encoder-decoder architecture"></p>

<h4 id="language-model-and-sequence-generation">Language model and sequence generation</h4>

<p>RNN can be used for NLP tasks, e.g. in speech recognition to calculate for words that sound the same (homophones) the probability for each writing variant. Such tasks usually require large corpora of text which is tokenized. A token can be a word, a sentence or also just a single character. The most common words could then be kept in a dictionary and vectorized using one-hot encoding.  Those word vectors could then be used to represent sentences as a matrix of word vectors. A special vector for the <em>unknown word</em> (<code class="highlighter-rouge">&lt;unk&gt;</code>) could be defined for words in a sentece that is not in the dictionary plus an <code class="highlighter-rouge">&lt;EOS&gt;</code> vector to indicate the end of a sentence.</p>

<p>The RNN can then calculate in each step the probabilities for each word appearing in the given context using softmax. This means if the dictionary contains the 10’000 most common words the prediction <script type="math/tex">\hat{y}</script> would be a vector of dimensions <script type="math/tex">(10'000 \times 1</script> containing the probabilities for each word. This probabaility is calculate using Bayesian probability given the already seen previous words:</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{y}^{<t>} = P(x^{<t>} \vert x^{<t-1>}, x^{<t-2>}, ... x^{<1>} ) %]]></script>

<p>This output vector indicates the probability distribution over all words given a sequence of <script type="math/tex">t</script> words. Predictions can be made until the <code class="highlighter-rouge">&lt;EOS&gt;</code> token is processed or until some number of words have been processed. Such a network could be trained with the loss function (<script type="math/tex">\ref{loss}</script>) and the cost function (<script type="math/tex">\ref{cost}</script>) to predict the next word for a given sequence of words. This also works on character level where the next character is predicted to form a word.</p>

<h3 id="vanishing-gradients-in-rnn">Vanishing Gradients in RNN</h3>

<p>Vanishing Gradients are also a problem for RNN. This is especially relevant for language models because a property is that sentences can have relationships between words spanning over a lot of words. Consider the following sequence of tokens representint the sentence <em>The cat, which already ate a lot of food, which was delicious, was full.</em>:</p>

<p><code class="highlighter-rouge">&lt;the&gt; &lt;cat&gt; &lt;which&gt; &lt;already&gt; &lt;ate&gt; &lt;a&gt; &lt;lot&gt; &lt;of&gt; &lt;food&gt; &lt;which&gt; &lt;was&gt; &lt;delicious&gt; &lt;was&gt; &lt;full&gt; &lt;EOS&gt;</code></p>

<p>Note that the token <code class="highlighter-rouge">&lt;was&gt;</code> affects the token <code class="highlighter-rouge">&lt;cat&gt;</code>. However, since there are a lot of tokens in between, the RNN will have a hard time predicting the token <code class="highlighter-rouge">&lt;was&gt;</code> correctly. To capture long-range dependencies between words the RNN would need to be very deep, which increases the risk of vanishing or exploding gradients. Exploding gradients can relatively easily be solved  by using <strong>gradient clipping</strong> where gradients are clipped to some arbitrary maximal value. Vanishing gradients are harder to deal with and require the use of <strong>gated recurrent units (GRU)</strong> to memorize words for long range dependencies.</p>

<h4 id="gated-recurrent-units">Gated Recurrent Units</h4>

<p><strong>Gated Recurrent Units (GRU)</strong> are a modification for the hidden layeres in an RNN that help mitigating the problem of vanishing gradients. GRU are cells in a RNN that have a memory which serves as an additional input to make a prediction. To better understand how GRU cells work, consider the following image depicting how a normal RNN cell works:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/rnn-cell.png" alt="Calculations in an RNN cell">
	<figcaption>Calculations in an RNN cell (Credits: Coursera)</figcaption>
</figure>

<h5 id="simple-gru">Simple GRU</h5>
<p>GRU units have a memory cell <script type="math/tex">% <![CDATA[
c^{<t>} %]]></script> to “remember” e.g. that the token <code class="highlighter-rouge">&lt;cat&gt;</code> was singular for later time steps. Note that for GRU cells <script type="math/tex">% <![CDATA[
c^{<t>} = a^{<t>} %]]></script> but we still use the variable <script type="math/tex">c</script> for consistency reasons, because in another type of cell (the {LSTM-cell]{#lstm-cells} which will be explained next) we use the same symbol. In each time step a value <script type="math/tex">\tilde{c}</script> is calculated as a candidate to replace the existing content of the memory cell <script type="math/tex">c</script>. This candidate uses an activation function (e.g. <script type="math/tex">\tanh</script>), its own trainable parameter matrix <script type="math/tex">W_c</script> and a separate bias <script type="math/tex">b_c</script>.</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\tilde{c}^{<t>} = \tanh \left( W_c \left[ c^{<t-1>}, x^{<t>} \right]  + b_c \right)
\label{cell_candidate}
\end{equation} %]]></script>

<p>After calculating the candidate <script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} %]]></script> we use an <strong>update-gate</strong> <script type="math/tex">\Gamma_u</script> to decide whether we should update the cell with this value or keep the old value. The value for <script type="math/tex">\Gamma_u</script> can be calculated using another trainable parameter matrix <script type="math/tex">W_u</script> and bias <script type="math/tex">b_u</script>. Because Sigmoid is used as the activation function, the values for <script type="math/tex">\Gamma_u</script> are always between 0 and 1 (for simplification you can also thing of <script type="math/tex">\Gamma_u</script> to be either exactly 0 or exactly 1).</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\Gamma_u = \sigma\left(  W_u \left[ c^{<t-1>, x^{<t>}} \right] + b_u \right)
\label{update_gate}
\end{equation} %]]></script>

<p>This gate is the key component of a GRU because it “decides” when to update the memory cell. Combining equations (<script type="math/tex">\ref{cell_candidate}</script>) and (<script type="math/tex">\ref{update_gate}</script>) gives us the following formula to calculate the value of the memory cell in each time step:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>}
\label{gru_update}
\end{equation} %]]></script>

<p>Note that the dimensions of <script type="math/tex">% <![CDATA[
c^{<t>} %]]></script>, <script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} %]]></script> and <script type="math/tex">\Gamma_u</script> corresponds to the number of units in the hidden layer. The asterisks <script type="math/tex">*</script> denote element-wise multiplication. The following picture illustrates the calculations inside a GRU cell. The black box stands for the calculations in formula <script type="math/tex">\ref{gru_update}</script>.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/gru-cell.png" alt="Calculations in a GRU cell">
	<figcaption>Calculations in a GRU cell</figcaption>
</figure>

<h5 id="full-gru">Full GRU</h5>

<p>The above explanations described a simplified version of a GRU with only one gate <script type="math/tex">\Gamma_u</script> to decide whether to update the cell value or not. Because the memory-cell <script type="math/tex">c</script> is a vector with several components you can think of it as a series of bits, whereas each bit remembers one specific feature about the already seen words (e.g. one bit for the fact that <code class="highlighter-rouge">&lt;cat&gt;</code> was singular, another bit to remember that the middle sentence was about food etc…). Full GRUs however usually have an additional parameter <script type="math/tex">\Gamma_r</script> that describes the relevance of individual features, which again uses its own parameter matrix <script type="math/tex">W_r</script> and bias <script type="math/tex">b_r</script> to be trained:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\Gamma_r = \sigma\left(  W_r \left[ c^{<t-1>, x^{<t>}} \right] + b_r \right)
\label{relevance_gate}
\end{equation} %]]></script>

<p>In short, GRU cells allow a RNN to remember things by using a memory cell which is updated depending on an update-gate <script type="math/tex">\Gamma_u</script>. In researach, the symbols used to denote the memory cell <script type="math/tex">c</script>, the candidate <script type="math/tex">\tilde{c}</script> and the two gates <script type="math/tex">\Gamma_u</script> and <script type="math/tex">\Gamma_r</script> are sometimes different. The following table contains all the parameters of a full GRU cell including a description and how to calculate them:</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>alternative</th>
      <th>Description</th>
      <th>Calculation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} %]]></script></td>
      <td><script type="math/tex">\tilde{h}</script></td>
      <td>candidate to replace the memory cell value</td>
      <td><script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} = \tanh\left(W_c \left[ \Gamma_r * c^{<t-1>}, x^{<t>} \right] + b_c \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\Gamma_u</script></td>
      <td><script type="math/tex">u</script></td>
      <td>Update-Gate to control whether to update the memory cell or not</td>
      <td><script type="math/tex">% <![CDATA[
\Gamma_u = \sigma\left( W_u \left[ c^{<t-1>}, x^{<t>} \right] + b_u \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\Gamma_r</script></td>
      <td><script type="math/tex">r</script></td>
      <td>Relevance-Gate to control the relevance of the memory cell values for the candidate</td>
      <td><script type="math/tex">% <![CDATA[
\Gamma_r = \sigma\left( W_r \left[ c^{<t-1>}, x^{<t>} \right] + b_r \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} %]]></script></td>
      <td><script type="math/tex">h</script></td>
      <td>new memory cell value at time step <script type="math/tex">t</script>
</td>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + (1 - \Gamma_u) * c^{<t-1>} %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
a^{<t>} %]]></script></td>
      <td>-</td>
      <td>new activation value</td>
      <td><script type="math/tex">% <![CDATA[
a^{<t>} = c^{<t>} %]]></script></td>
    </tr>
  </tbody>
</table>

<h4 id="long-short-term-memory-units">Long Short Term Memory Units</h4>

<p>An advanced alternative to GRU are <strong>Long Short Term Memory (LSTM)</strong> cells. LSTM cells can be considered a more general and more powerful version of GRU cells. Such cells also use a memory cell <script type="math/tex">c</script> to remember something. However, the update of this cell is slightly different from GRU cells.</p>

<p>In contrast to GRU cells, the memory cell does not correspond to the activation value anymore, so for LSTM-cells <script type="math/tex">% <![CDATA[
c^{<t>} \neq a^{<t>} %]]></script>. It also does not use a relevance gate <script type="math/tex">\Gamma_r</script>
anymore but rather a <strong>forget-gate</strong> <script type="math/tex">\Gamma_f</script> that governs whether to forget the current cell value or not. Finally, there is a third parameter <script type="math/tex">\Gamma_o</script> to act as <strong>output-gate</strong> and is used to scale the update memory cell value to calculate the activation value for the next iteration.</p>

<p>The following table summarizes the different parameters and how to calculate them.</p>

<table>
  <thead>
    <tr>
      <th>Symbol</th>
      <th>alternative</th>
      <th>Description</th>
      <th>Calculation</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} %]]></script></td>
      <td><script type="math/tex">\tilde{h}</script></td>
      <td>candidate to replace the memory cell value</td>
      <td><script type="math/tex">% <![CDATA[
\tilde{c}^{<t>} = \tanh\left(W_c \left[ a^{<t-1>}, x^{<t>} \right] + b_c \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\Gamma_u</script></td>
      <td><script type="math/tex">u</script></td>
      <td>Update-Gate to control update of memory cell</td>
      <td><script type="math/tex">% <![CDATA[
\Gamma_u = \sigma\left( W_u \left[ a^{<t-1>}, x^{<t>} \right] + b_u \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\Gamma_f</script></td>
      <td><script type="math/tex">u</script></td>
      <td>Forget-Gate to control influence of current memory cell value for the new value</td>
      <td><script type="math/tex">% <![CDATA[
\Gamma_f = \sigma\left( W_f \left[ a^{<t-1>}, x^{<t>} \right] + b_f \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">\Gamma_o</script></td>
      <td><script type="math/tex">u</script></td>
      <td>Output-Gate to control influence of current memory cell value for the new value</td>
      <td><script type="math/tex">% <![CDATA[
\Gamma_o = \sigma\left( W_o \left[ a^{<t-1>}, x^{<t>} \right] + b_o \right) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} %]]></script></td>
      <td><script type="math/tex">h</script></td>
      <td>new memory cell value at time step <script type="math/tex">t</script>
</td>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} = \Gamma_u * \tilde{c}^{<t>} + \Gamma_f * c^{<t-1>} %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
a^{<t>} %]]></script></td>
      <td>-</td>
      <td>new activation value</td>
      <td><script type="math/tex">% <![CDATA[
a^{<t>} = \Gamma_o * \tanh(c^{<t>}) %]]></script></td>
    </tr>
  </tbody>
</table>

<p>The following image illustrates how calculations are done in an LSTM cell:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/lstm-cell.png" alt="Calculations in an LSTM cell">
	<figcaption>Calculations in an LSTM cell</figcaption>
</figure>

<p>Several of such LSTM cells can be combined to form an <strong>LSTM-network</strong>. There are variations for the LSTM cell implementation, such as making the gate parameters <script type="math/tex">\Gamma_u</script>, <script type="math/tex">\Gamma_f</script> and <script type="math/tex">\Gamma_o</script> not only depend on the previous activation value <script type="math/tex">% <![CDATA[
a^{<t-1>} %]]></script> and the current input token <script type="math/tex">% <![CDATA[
x^{<t>} %]]></script>, but also on the previous memory cell valu <script type="math/tex">% <![CDATA[
c^{<t-1>} %]]></script>. The update for the update gate is then <script type="math/tex">% <![CDATA[
\Gamma_u = \sigma\left( W_u \left[ a^{<t-1>}, x^{<t>}, c^{<t-1>} \right] + b_u \right) %]]></script> (other gates analogous). This is called a <strong>peephole connection</strong>.</p>

<h4 id="gru-vs-lstm">GRU vs. LSTM</h4>

<p>There is not an universal rule when to use GRU- or LSTM-cells. GRU cells represent a simpler model, hence they are more suitable to build a bigger RNN model because they are computationally more efficient and the RNN will scale faster. On the other hand the LSTM-cells are more powerful and more flexible, but they also require more training data. In case of doubt, try LSTM cells because they have sort of become state of the art for RNN.</p>

<h3 id="bidirectional-rnns">Bidirectional RNNs</h3>

<p>Unidirectional RNNs only consider already seen tokens at a time step <script type="math/tex">% <![CDATA[
<t> %]]></script> to make a prediction. In contrast, <strong>bidirectional RNN (BRNN)</strong> also take <em>subsequent</em> tokens into account. This is for example helpful for NER when trying to predict whether the word <em>Teddy</em> is part of a name in the following two sencences:</p>

<p><code class="highlighter-rouge">&lt;he&gt; &lt;said&gt; &lt;teddy&gt; &lt;bears&gt; &lt;are&gt; &lt;on&gt; &lt;sale&gt; &lt;EOS&gt;</code></p>

<p><code class="highlighter-rouge">&lt;he&gt; &lt;said&gt; &lt;teddy&gt; &lt;roosevelt&gt; &lt;was&gt; &lt;a&gt; &lt;great&gt; &lt;president&gt; &lt;EOS&gt;</code></p>

<p>Just by looking at the previously seen words it is not clear at time step <script type="math/tex">t=3</script> whether <code class="highlighter-rouge">&lt;teddy&gt;</code> is part of a name or not. To do that we need the information of the following tokens. A BRNN can do this using an additional layer. During forward propagation the activation values <script type="math/tex">\overrightarrow{a}</script> are computed as seen above from the input tokens and the previous activation values using an RNN cell (normal RNN cell, GRU or LSTM). The second part of forward propagation calculates the values <script type="math/tex">\overleftarrow{a}</script> from left to right using the additional layer. The following picture illustrates this. Note that the arrows in blue and green only indicate the order in which the tokens are evaluated. It does not indicate backpropagation.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/brnn.png" alt="Forward- and backpropagation in a bidirectional RNN">
	<figcaption>Forward- and backpropagation in a bidirectional RNN</figcaption>
</figure>

<p>After a single pass of forward propagation a prediction at time step <script type="math/tex">t</script> can be made by stacking the activations of both directions and calculating the prediction value as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\hat{y}^{<t>} = g \left( W_y \left[ \overrightarrow{a}^{<t>}, \overleftarrow{a}^{<t>} \right] + b_y \right) %]]></script>

<p>The advantage of BRNN is that it allows to take into account words from both directions when making a prediction, which makes it a good fit for many language-related applications like machine translation. On the downside, because tokens from both directions are considered, the whole sequence needs to be processed before a prediction can be made. This makes it unsuitable for tasks like real-time speech recognition.</p>

<h3 id="deep-rnn">Deep RNN</h3>

<p>The RNNs we have seen so far consisted actually of only one layer (with the exception of the BRNN which used an additional layer for the reverse direction). We can however stack several of those layers on top of each other to get a <strong>Deep RNN</strong>. In such a network, the results from one layer are passed on to the next layer in each time step <script type="math/tex">t</script>:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/deep-rnn.png" alt="Example of a Deep-RNN">
	<figcaption>Example of a Deep-RNN</figcaption>
</figure>

<p>The activation <script type="math/tex">% <![CDATA[
a^{[l]<t>} %]]></script> for layer <script type="math/tex">l</script> at time step <script type="math/tex">t</script> can be calculated as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
a^{[l]<t>} = g\left( W_a^{[l]} \left[ a^{[l]<t-1>}, a^{[l-1]<t>} \right] + b_a^{[l]}\right) %]]></script>

<p>Deep-RNN can become computationally very expensive quickly, therefore they usually do not contain as many stacked layers as we would expect in a conventional Deep-NN.</p>

<h2 id="natural-language-processing">Natural Language Processing</h2>

<p>In <a href="#language-model-and-sequence-generation">the chapter about language modelling</a> above we have seen that we can represent words from a dictionary as vectors using one-hot encoding where all components are zero except for one.</p>

<p><img src="/assets/img/articles/ml/dl_5/words-one-hot.png" alt="one-hot word vectors"></p>

<p>The advantage of such an encoding is that the calculation of a word vector and looking up a word given its vector is easy. On the other hand this form of encoding does not contain any information about the relationships of words between each other. An alternative sort of word vectors are <strong>word embeddings</strong>. In such vectors, each component of a vector reflects a different feature of a word meaning (e.g. age, sex, food/non-food, word type, etc…). Therefore the components can all have non-null values. Words that are semantically similar have similar values in the individual components. For visualization we could also reduce dimensionality to two (or three) dimensions, e.g. by applying the <a href="#t-sne">t-SNE algorithm</a>. By doing so it turns out that words with similar meanings are in similar positions in vector space.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/vector-space.png" alt="words in vector space">
	<figcaption>Words in vector space (Credits: Coursera)</figcaption>
</figure>

<h3 id="properties-of-word-embeddings">Properties of word embeddings</h3>

<p>Word embeddings have become hugely popular in NLP and can for example be used for NER. Oftentimes an existing model can be adjusted for a specific task by performing additional training on  suitable training data (transfer learning). This training set and also the dimensionality of the word vectors can be much smaller. The relevance of a word embedding <script type="math/tex">e</script> is simliar to the vector of a face in face recognition in computer vision: It is a vectorized representation of the underlying data. An important distinction however is that in order to get word embeddings a model needs to learn a fixed-size vocabulary. Vectors for words outside this vocabulary can not be calculated. In contrast a CNN could calculate a vector for a face it has never seen before.</p>

<p>Word embeddings are useful to model analogies and relationships between words. The best known example for this is the one from <a href="https://arxiv.org/abs/1310.4546">the original paper</a>:</p>

<script type="math/tex; mode=display">\begin{equation}
e_{man} - e_{woman} \approx e_{king} - e_{queen}
\label{word2vec_1}
\end{equation}</script>

<p>The distance between the vectors for “man” and “woman” is similar to the distance between the vectors for “king” and “queen”, because those two pairs of words are related in the same way. We can also observe that a trained model has learned the relationship between these two pairs of words because the vector representations of their distances is approximately parallel. This also applies to other kinds of word pairings, like verbs in different tenses or the relationship between a country and its capital:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/word-embeddings.png" alt="word embeddings">
	<figcaption>Example of word embeddings (Credits: <a href="https://www.tensorflow.org/tutorials/word2vec" target="_blank">Tensorflow</a>)</figcaption>
</figure>

<p>Therefore we could get the following equation by rearranging formula <script type="math/tex">\ref{word2vec_1}</script>:</p>

<script type="math/tex; mode=display">\begin{equation}
e_{king} - e_{man} + e_{woman} \approx e_{queen}
\label{word2vec_2}
\end{equation}</script>

<p>This way the word embedding for “queen” can be calculated using the embeddings of the other words. To get the word for its embedding we can use a similarity function <script type="math/tex">sim</script>, which measures the similarity between two embeddings <script type="math/tex">u</script> and <script type="math/tex">v</script>. Often the <strong>cosine similarity</strong> is used for this function:</p>

<script type="math/tex; mode=display">sim(u,v) = \frac{u^T v}{\lVert u\rVert_2 \lVert v \rVert_2}</script>

<p>With the help of the similarity function we can find the word for “queen” by comparing the embedding <script type="math/tex">e_{queen}</script> against the embeddings of all other word from the vocabulary:</p>

<script type="math/tex; mode=display">w = \underset{w}{\operatorname{argmax}} sim(e_{queen}, e_{king} - e_{man} + e_{woman})</script>

<h3 id="embedding-matrix">Embedding matrix</h3>
<p>The embeddings of the words in the vocabulary can be precomputed and stored in an <strong>embedding matrix</strong> <script type="math/tex">*</script>. This is efficient because the learned embeddings don’t need to be computed each time. The embedding <script type="math/tex">e_j</script> of a word <script type="math/tex">j</script> from the vocabulary can easily be retrieved by multiplying its one-hot encoding <script type="math/tex">O_j</script> with the embedding matrix:</p>

<script type="math/tex; mode=display">\begin{equation}
e_j = E \cdot O_j
\label{embedding_matrix}
\end{equation}</script>

<p>However, since most components in <script type="math/tex">O_j</script> are zeroes, a lot of multiplications are done for nothing. Therefore a specialized function is normally used in practice to get the embedding for a word.</p>

<h3 id="word2vec">Word2Vec</h3>

<p><strong>Word2Vec</strong> (W2V) is the probably most popular implementation for word embeddings. W2V contains two approaches:</p>

<ul>
  <li>Skip-Gram</li>
  <li>CBOW (Continuous Bag Of Words)</li>
</ul>

<h4 id="skip-gram">Skip-Gram</h4>
<p>In the skip-gram approach a random word is chosen as context word during training to learn word embeddings. Usually the context words are not chosen with uniform random distribution but according to their frequency in the corpus. Frequent words have a lower probability for being selected as context words. After that a window of e.g. 5 words (i.e. the 5 words before and after) can be defined, from which a target word is sampleed. The learning problem then consists of predicting the target word from the context word, i.e. learning a mapping from the context a the target word.</p>

<p>Consider for example a dictionary of 10’000 words and the two words “orange” (context word) and “juice” (target word, to be learned) as training tuple. For these two words their embeddings <script type="math/tex">e_c</script> and <script type="math/tex">e_t</script> can be retrieved as shown in (<script type="math/tex">\ref{embedding_matrix}</script>). The embedding <script type="math/tex">e_c</script> can be fed to a softmax unit which calculates a prediction for the target word embedding <script type="math/tex">e_t</script> as follows:</p>

<script type="math/tex; mode=display">p(t \vert c) = \frac{e^{\Theta^T_t e_c}}{ \sum_{j=1}^{10'000} e^{\Theta^T_j e_c} }</script>

<p>The output vector <script type="math/tex">\hat{y}</script> is then a vector with probabilities for all 10’000 words. The training goal should therefore optimize the parameter <script type="math/tex">\Theta_t</script> for the target word so that the probability for <script type="math/tex">e_t</script> (“juice”) is high. The loss function is then as usual the negative log-likelyhood (note that <script type="math/tex">y</script> is the one-hot encoding for the target word, in our example “juice”):</p>

<script type="math/tex; mode=display">\mathcal{L} (\hat{y} ,y) = - \sum_{i=1}^{10'000} y_i \log(\hat{y}_i)</script>

<p>Skip-Gram can calculate very good word embeddings. The problem with softmax however is its computational speed. Every time we want to evaluate the probability <script type="math/tex">p(t \vert c)</script> we need to carry out a sum of over all words in the vocabulary. This will not scale well with increasing vocabulary size. To solve this we could apply <strong>hieararchical softmax</strong> which splits up the computed vector for the cell value by recursively dividing it into halves until the maximum value has been found (divide and conquer).</p>

<h4 id="negative-sampling">Negative Sampling</h4>

<p>A more performant way of calculating word embeddings is <strong>negative sampling</strong>, which can be regarded as a modified learning problem than the one used in Skip-Gram. In negative sampling, a training set of <script type="math/tex">k+1</script> samples is created for each valid combination of context and target word by deliberately creating <script type="math/tex">k</script> negative samples.</p>

<p>The learning problem in negative sampling is therefore constructed by creating a pair of words by randomly defining a context word and sampling a target word from a window to get a valid target word. This pair is labelled “1” because it is a valid combination of context and target word. After that, <script type="math/tex">k</script> additional target words are sampled at random from the vocabulary. Those will be labelled “zero” because they are considered non-target words (even if they happen to appear inside the window!). The value for <script type="math/tex">k</script> should be between 5 and 20 for smaller datasets and between 2 and 5 for larger datasets.</p>

<p>This way we get a training set of <script type="math/tex">k+1</script> pairs of words. We can use this to learn a mapping from context to target word by treating the problem as a binary classification problem. In each iteration we train a NN with only these <script type="math/tex">k+1</script> word pairs from the dataset (in contrast to Skip-Gram where the training is done over the whole vocabulary). The probability an arbitrary target word <script type="math/tex">t'</script> and the context word $$c$ co-occurring is defined as follows:</p>

<script type="math/tex; mode=display">P(y=1 \vert c,t') = \sigma (\Theta_c^T e_c)</script>

<p>The learning problem is therefore to reduce the parameters <script type="math/tex">\Theta_c</script> so that the cost is minimal (i.e. <script type="math/tex">P(y=1 \vert c,t) \approx 1</script>).</p>

<h3 id="glove">GloVe</h3>

<p>An alternative to W2V is <strong>GloVe</strong>, which (although not as popular as W2V) has some advantages over W2V due to its simplicity. The GloVe algorithm counts for a given word <script type="math/tex">i</script> the co-occurrence of each other word <script type="math/tex">j</script> in a certain context. The notion of context can be defined arbitrarily, e.g. by defining a window like in Skip-Gram. This means for each word <script type="math/tex">j</script> a value <script type="math/tex">x_{ij}</script> is calculated.</p>

<p>The learning problem is then defined by minimizing the following function (again for a vocabulary of 10’000 words):</p>

<script type="math/tex; mode=display">\begin{equation}
minimize \sum_{i=1}^{10'000} \sum_{j=1}^{10'000} f(x_{ij}) (\Theta_i^T e_j + b_i + b'_j - \log x_{ij})^2
\label{glove_min}
\end{equation}</script>

<p>The function <script type="math/tex">f(x_{ij})</script> calculates a weighting term for the individual values of <script type="math/tex">x_{ij}</script> with the following properties:</p>

<ul>
  <li>If <script type="math/tex">x_{ij}=0</script> (word <script type="math/tex">j</script> does not co-occur with word <script type="math/tex">i</script> then <script type="math/tex">f(x_{ij}) = 0</script>. This prevents the above formulat to calculate <script type="math/tex">\log(x_{ij}) = \infty</script>. In other words: by multiplying with <script type="math/tex">f(x_{ij})</script> we only sum up the values of <script type="math/tex">x_{ij}</script> for pairs of words that actually co-occur at least once.</li>
  <li>more frequent words get a higher weight than less frequent words. At the same time the weight is not too large to prevent stop-words from having too much influence.</li>
  <li>less frequent words get a smaller weight than more frequent words. At the same time the weight is not too small so that even rare words have some sensible influence.</li>
</ul>

<p>With this learning problem the terms <script type="math/tex">\Theta_i</script> and <script type="math/tex">e_j</script> are symmetric, i.e. they end up with the same optimization objective. Therefore the embedding for a given word <script type="math/tex">w</script> can be calculated by taking the average of <script type="math/tex">e_w</script> and <script type="math/tex">\Theta_w</script>.</p>

<p>It turns out that even with such a simple function to minimize as seen in (<script type="math/tex">\ref{glove_min}</script>) good word embeddings can be learned. This simplicity compared to W2V is somewhat surprising and might be a reason for GloVe to be popular for some researchers.</p>

<h3 id="sentiment-classification">Sentiment classification</h3>

<p><strong>Sentiment classification</strong> (SC) is the process of deciding from a text whether the writer likes or dislikes something. This is for example required to map textual reviews to star-ratings (1 star=bad, 5 stars=great).</p>

<p>The learning problem in SC is to learn a function which maps an input <script type="math/tex">x</script> (e.g. a restaurant review) to a discrete output <script type="math/tex">y</script> (e.g. a star-rating). Therefore the learning problem is a multinomial classification problem where the predicted class is the number of stars. For such learning problems, however, training data is usually sparse.</p>

<p>A simple classifier could consist of calculating the word embeddings for each word in the review and calculating their average. This average vector could then be fed into a softmax classifier which calculates the probability for each of the target classes. This also works for long reviews because the average vector will always have the same dimensionality. However, by averaging the word vectors we lose information about the order of the words. This is important because the word sequence “not good” should have a negative impact on the star-rating whereas when calculating the average of “not” and “good” individually the negative meaning is lost and the star-rating would be influenced positively. An alternative would therefore be to train an RNN with the word embeddings.</p>

<h3 id="debiasing-word-embeddings">Debiasing Word Embeddings</h3>

<p>Word Embeddings can suffer from bias depending on the training data used. The term <em>bias</em> denotes bias towards gender/race/age/etc… here, not the numeric value often seen before. Such stereotypes can become a problem because they can enforce stereotypes by learning inappropriate relationship between words (e.g. <em>man</em> is to <em>computer programmer</em> as <em>woman</em> is to <em>homemaker</em>). To neutralize such biases, you could perform the following steps:</p>

<ol>
  <li>
<strong>Identify bias direction</strong>: If for example we want to reduce the gender bias we could define pairs of male and female forms of words and average the difference between their embeddings (e.g. <script type="math/tex">frac{e_{he} - e_{she}}{2}</script>). The resulting vector gives us the bias direction <script type="math/tex">g</script>.</li>
  <li>
<strong>Neutralize</strong>: For every word that is not definitional, project to get rid of bias. Definitional means that the gender is important for the meaning of the word. An example for a definitional word is <em>grandmother</em> or <em>grandfather</em>, because here the gender information cannot be omitted without losing semantic meaning.
We can compute the neutralized embedding as follows:</li>
</ol>

<script type="math/tex; mode=display">e^{bias\_component} = \frac{e \cdot g}{||g||_2^2} * g \\
e^{debiased} = e - e^{bias\_component}</script>

<p>The figure below should help you visualize what neutralizing does. If you’re using a 50-dimensional word embedding, the 50 dimensional space can be split into two parts: The bias-direction  <script type="math/tex">g</script> , and the remaining 49 dimensions, which we’ll call <script type="math/tex">g_{\perp}</script>. In linear algebra, we say that the 49 dimensional <script type="math/tex">g_{\perp}</script> is perpendicular (or “othogonal”) to <script type="math/tex">g_{\perp}</script>, meaning it is at 90 degrees to  gg . The neutralization step takes a vector such as <script type="math/tex">e_{receptionist}</script> and zeros out the component in the direction of  gg , giving us <script type="math/tex">e_{receptionist}^{debiased}</script> .
Even though <script type="math/tex">g_{\perp}</script> is 49 dimensional, given the limitations of what we can draw on a screen, we illustrate it using a 1 dimensional axis below.</p>

<p><img src="/assets/img/articles/ml/dl_5/debiasing-neutralize.png" alt="Neutralizing"></p>

<ol>
  <li>
<strong>Equalize pairs</strong>: Equalization is applied to pairs of words that you might want to have differ only through the gender property. As a concrete example, suppose that “actress” is closer to “babysit” than “actor.” By applying neutralizing to “babysit” we can reduce the gender-stereotype associated with babysitting. But this still does not guarantee that “actor” and “actress” are equidistant from “babysit.” The equalization algorithm takes care of this.
The key idea behind equalization is to make sure that a particular pair of words are equi-distant from the 49-dimensional <script type="math/tex">g_\perp</script>. The equalization step also ensures that the two equalized steps are now the same distance from  <script type="math/tex">e_{receptionist}^{debiased}</script>, or from any other work that has been neutralized. In pictures, this is how equalization works:</li>
</ol>

<p><img src="/assets/img/articles/ml/dl_5/debiasing-equalize.png" alt="Equalizing"></p>

<p>In the above steps we used gender bias as an example, but the same steps can be applied to eliminate other types of bias too. Word embedding will almost alway suffer from bias that is intrinsically contained in the corpora they wore learned from.</p>

<h2 id="sequence-to-sequence-models">Sequence-to-sequence models</h2>

<p>We have <a href="#rnn-architectures">learned</a> that RNNs with a <em>many-to-many</em> architecture (with <em>encode-decoder</em> networks as a special form) take a sequence as an input and also produce a sequence as their output. Such models are called <strong>Sequence-to-Sequence (S2S)</strong> models. Such networks are traditionally used in <strong>machine translation</strong> where the input consists of sentences, which are transformed by an encoder network to serve as the input for the decoder network which does the actual translation. The output is then again a sequence, namely the translated sentence. The same process works likewise for other data. You could for instance take an image as an image as input and have an RNN try to produce a sentence that states what is on the picture (<strong>image captioning</strong>). The encoder network in this case could then be a conventional CNN whereas the last layer will contain the image as a vector. This vector is then served to an RNN which acts as the decoder network to make predictions. Assuming you have enough already captioned images as training data an RNN could then learn how to produce captions for yet unseen images.</p>

<h3 id="s2s-in-machine-translation">S2S in machine translation</h3>
<p>There area certain similarities between S2S-models and the previously seen <a href="#language-models-and-sequence-generation">language-models</a> where we had an RNN produce a sequence of words based on the probability of previously seen words in the sequence:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/language-model.png" alt="example structure of a language model">
	<figcaption>Example structure of a language model (Credits: Coursera)</figcaption>
</figure>

<p>In such a setting the output was generated by producing a somewhat random sequence of words. However, that is not what we want in machine translation. Here we usually want the most likely sentence that corresponds to the best translation for a given input sentence. This is a key difference between language models as seen before and machine translation models. So in contrast to the model above, a machine translation model does not start with the zero vector as the first token, but rather takes a whole sentence and encodes it using the encoder part of the network.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/machine-translation.png" alt="example structure of a machine translation model">
	<figcaption>Example structure of a machine translation model (Credits: Coursera)</figcaption>
</figure>

<p>In that respect one can think of a machine translation model as kind of <strong>conditional language model</strong> that calculates the probability of a translated sentence <em>given the input sentence in the original language</em>.</p>

<h3 id="beam-search">Beam search</h3>

<p>Suppose we have the following french sentence as an input:</p>

<p><em><code class="highlighter-rouge">Jane visite l'Afrique en septembre.</code></em></p>

<p>Possible translations to English for this sentence could be:</p>

<ul>
  <li><em><code class="highlighter-rouge">Jane is visiting Africa in September.</code></em></li>
  <li><em><code class="highlighter-rouge">Jane is going to be visiting Africa in September.</code></em></li>
  <li><em><code class="highlighter-rouge">In September, Jane will visit Africa.</code></em></li>
  <li><em><code class="highlighter-rouge">Her African friend welcomed Jane in September.</code></em></li>
</ul>

<p>Each of these sentences is a more or less adequate translation of the input sentence. One possible approach to calculate the most likely sentence (i.e. the best translation) is going through the words one by one to calculate the joint probability of a sentence by always taking the most probable word in each step as the next token. This approach is called <strong>greedy search</strong> and although it is fairly simple, it does not work well for machine translation. This is because small differences in probability for earlier words can have a big impact on what sentence is picked after inspection of all tokens. By using greedy search we would also try to find the best translation in a set of exponentially many sentences. Considering a vocabulary of 10’000 words and a sentence length of 10 words this would yield a solution space of <script type="math/tex">$10'000^{10}</script> theoretically possible sentences.</p>

<p>Another approach which works reasonably well is <strong>beam search (BS)</strong>. BS is an approximative approach, i.e. it tries to find a <em>good enough</em> solution. This has computational advantages over exact approaches like <a href="https://en.wikipedia.org/wiki/Breadth-first_search">BFS</a> or <a href="https://en.wikipedia.org/wiki/Depth-first_search">DFS</a>, meaning the algorithm runs much faster but does not guarantee to find the exact optimum.</p>

<p>BS also goes through the sequence of words one by one. But instead of choosing the one most likely word as the next token in each step, it considers a number of alternatives. This number <script type="math/tex">B</script> is called the <strong>beam width</strong>. In each step, only the <script type="math/tex">B</script> most likely choices are kept as the next word. This means that a partial sequance is not further evaluated if it does not continue with one of the <script type="math/tex">B</script> most likely words. The following figure illustrates this:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/beam-search.png" alt="beam search">
	<figcaption>Example of beam search for B=3 (Credits: Coursera, with own adjustments)</figcaption>
</figure>

<p>The higher the value for <script type="math/tex">B</script> the more alternatives are considered in each step, hence the more combinatorial paths are followed and the computationally more expensive the algorithm becomes. With a value of <script type="math/tex">B=1</script> BS essentially becomes greedy search. Productive systems usually choose a value between 10 and 100 for <script type="math/tex">B</script>. Larger values like <script type="math/tex">% <![CDATA[
1000 < B < 3000 %]]></script> happen to be used for research purposes, although the final utility will decrease the bigger <script type="math/tex">B</script> becomes.</p>

<h4 id="refinements-to-beam-search">Refinements to Beam Search</h4>

<p>There are a few tricks that help improving BS performance- or result-wise:</p>

<ul>
  <li>Length normalization</li>
  <li>Error analysis</li>
</ul>

<h5 id="length-normalization">Length normalization</h5>

<p>We have seen BS maximizing the following probability:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align*}
\arg \max & \prod_{t=1}^{T_y} P(y^{<t>} \vert x, y^{<1>}, ..., y^{<t-1>} ) \\
          & = P(y^{<1>}, ..., y^{<T_y>} \vert x ) \\
          & = P(y^{<1>} \vert x) \cdot P(y^{<2>} \vert x, y^{<1>}) \cdot ... \cdot P(y^{<T_y>} \vert x, y^{<1>}, ..., y^{<T_y - 1>})
\end{align*} %]]></script>

<p>The problem with this is that the probabilities of the individual tokens are usually much smaller than 1 which results in an overall probability which might be too small to be stored by a computer (<em>numerical underflow</em>). Therefore, the following formula is used to keep the calculation numerically more stable and less prone to rounding errors:</p>

<script type="math/tex; mode=display">% <![CDATA[
\arg \max \sum_{y=1}^{T_y} \log P(y^{<t>} \vert x, y^{<1>}, ..., y^{<T_y -1>}) %]]></script>

<p>Because the logarithm is a strictly monotonically increasing function maximizing the product of probabilities is the same as maximizing the sum of their logarithms. However, this formula will still result in shorter sequences because the overall probability for a sentence will only decrease the longer it gets. Longer sequences are therefore penalized while shorter sequences benefit from the higher probability. To fix this, a normalization parameter <script type="math/tex">\alpha</script> is introduced and the term is normalized by dividing by a power of the number of words in the sequence (<strong>length normalization</strong>):</p>

<script type="math/tex; mode=display">% <![CDATA[
\arg \max \frac{1}{T_y^\alpha} \sum_{y=1}^{T_y} \log P(y^{<t>} \vert x, y^{<1>}, ..., y^{<T_y -1>}) %]]></script>

<p>The hyperparamter <script type="math/tex">\alpha</script> controls the degree of normalization. This means instead of comparing the raw values for the probability of a partial sequence, Beam Search will compare the normalized values. For values of <script type="math/tex">\alpha \rightarrow 1</script> the term is completely normalized, for values of <script type="math/tex">\alpha \rightarrow 0</script> no normalization is done.</p>

<h5 id="error-analysis">Error Analysis</h5>

<p>If a system does not output sequences with the desired quality we have to examine where causes for this are possibly rooted: in the RNN model itself or in the Beam Search algorithm.</p>

<p>Consider the French sentence <em>“Jane visite l’Afrique en septembre”</em> from above. We now compare this sentence to the following sentences:</p>

<ul>
  <li>
<script type="math/tex">y^*</script>: optimal translation by a humen (e.g. <em>“Jane visits Africa in September.”</em>
</li>
  <li>
<script type="math/tex">\hat{y}</script>: translation from the model (e.g. <em>“Jane visited Africa last September.”</em>
</li>
</ul>

<p>In this example it is evident that the model did not output a good translation. We can now distinguish the following cases for the probabilities of those two sentences (from the set of all possible sentences).</p>

<ul>
  <li>
<script type="math/tex">P(y^* \vert x > P(\hat{y} \vert x)</script>: In this case the algorithm chose the sentence with the lower probability. The cause for the error is therefore rooted in the search algorithm (Beam Search).</li>
  <li>
<script type="math/tex">P(y^* \vert x \leq P(\hat{y} \vert x)</script>: In this case the algorithm calculated a too small probability for the optimal translation. The cause for the error is therefore rooted in the model (RNN).</li>
</ul>

<p>To get a feeling for where most errors are rooted we could make this comparison for a number of samples and analyze where most errors are rooted.</p>

<h4 id="bleu-score">Bleu Score</h4>
<p>A special property of translation is that there are possibly many different translation which are considered equally correct. Since linguistics is not an exact science and language itself is somewhat fuzzy due to its ambiguousity, the evaluation of NLP tasks naturally contains a grey area where the distinction between correct or wrong is not clear.</p>

<p>One method to compare two texts (e.g. human and machine translation) is to calculate the <strong>Bleu-Score</strong> (<strong>B</strong>i<strong>l</strong>ingual <strong>e</strong>valuation <strong>u</strong>nderstudy). This value measures the quality of a translation as degree of overlap with a (or several) reference translation(s). A higher value means a better quality.</p>

<p>The Bleu score therefore measures the precision as the number of words in the translation that also appear in the reference. Note that to calculate recall we could use <a href="https://en.wikipedia.org/wiki/ROUGE_(metric)">Rouge</a>. Consider the following translations for the French sentence “<em>Le chat es sur le tapis.</em>”</p>

<table>
  <tbody>
    <tr>
      <td>Reference translation 1</td>
      <td><code class="highlighter-rouge">The cat is on the mat.</code></td>
    </tr>
    <tr>
      <td>Reference translation 2</td>
      <td><code class="highlighter-rouge">There is a cat on the mat.</code></td>
    </tr>
    <tr>
      <td>Translation</td>
      <td>
<code class="highlighter-rouge">the the the the the the the</code> (7 words)</td>
    </tr>
  </tbody>
</table>

<p>Because the word <code class="highlighter-rouge">the</code> appears in at least one of the reference translation, the resulting Bleu-score would be <script type="math/tex">\frac{7}{7} = 1</script> (i.e. a perfect translation). However, this is obviously wrong. Therefore a <strong>modified precision</strong> is used which limits the number of counted appearances to the maximum number of appearances in either one of the sentences. Because in the above example the word <code class="highlighter-rouge">the</code> appears twice in the first reference translation and once in the second reference translation, this maximum is 2 and the modified precision is therefore <script type="math/tex">\frac{2}{7}</script>.</p>

<p>Instead of looking at words in isolation (<em>unigrams</em>) we could also look at pairs of words (<em>bigrams</em>), triplets (<em>3-grams</em>) or generally tuples of any number. For this the machine translation sentence is split into its different bigrams which are then counted. We could then additionally calculate a <strong>clip count</strong> which indicates in how many of the reference sentences the bigram appears.</p>

<table>
  <thead>
    <tr>
      <th>Bigram</th>
      <th>Count</th>
      <th>Clip Count</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code class="highlighter-rouge">the cat</code></td>
      <td>2</td>
      <td>1</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">cat the</code></td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">cat on</code></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">on the</code></td>
      <td>1</td>
      <td>1</td>
    </tr>
    <tr>
      <td><code class="highlighter-rouge">the mat</code></td>
      <td>1</td>
      <td>1</td>
    </tr>
  </tbody>
</table>

<p>The modified <em>n-gram</em>-precision could then be calculated by dividing the sum of clipped counts by the sum of all counts:</p>

<script type="math/tex; mode=display">p_n = \frac{\sum_{\text{n-gram} \in \hat{y}} count_{clip}(\text{n-gram})}
{\sum_{\text{n-gram} \in \hat{y}} count(\text{n-gram})}</script>

<p>For the above example, the <script type="math/tex">p_2</script> value would be <script type="math/tex">\frac{4}{6} \approx 0.666</script>. A good translation has values for different n-grams close to 1. All n-gram-scores can be combined to the <strong>combined Bleu score</strong>:</p>

<script type="math/tex; mode=display">\exp \left( \frac{1}{4}  \sum_{n=1}^4 p_n \right)</script>

<p>Of course, again short translations have an advantage here because there are fewer possibilities for errors. An extreme example of this would be a translation in form of a single word which would get a score of 1 if only this words appears in the reference translations. To prevent this, an adjustment factor called <strong>brevity penalty</strong> (BP) is introduced, which penalizes short translations:</p>

<script type="math/tex; mode=display">% <![CDATA[
BP =
\begin{cases}
1   & \text{if lenghts are equal}\\
\exp\left( 1 - \frac{ \text{machine translation}_{\text{length}} }{ \text{reference translation}_{\text{length}} } \right)  & otherwise
\end{cases} %]]></script>

<h2 id="attention-models">Attention models</h2>
<p>So far the task of machine translation has only been exemplified with sequence models following an <em>encoder-decoder</em>-architecture where one RNN “reads” a sentence and encodes it as a vector and another RNN makes the translation. This works well for comparatively short sentences. However, for long sentences the performance (e.g. measured by Bleu score) will decrease. This is because instead start translating a very long sentence chunk by chunk (like a human would) it is difficult to make an RNN memorize the whole sentece because it is processed all in one go.</p>

<p>A modification to the <em>encoder-decoder</em>-architecture are <strong>attention models</strong> (AM). AM process a sentence similarly to how a human would by splitting it up into several chunks (<em>contexts</em>) of equal size and translating each chunk separately. This is especially useful in tasks with real-time requirements like speech recognition or simultaneous translation where you usually don’t want to wait for the whole input to be available before making a prediction. For each context <script type="math/tex">c</script> the model computes the amount attention it should pay to each word. The output for this chunk serves as input for the next chunk.</p>

<p>As an example consider the chunk “<em>Jane visite l’Afrique en septembre…</em>” from a much longer French sentence. This chunk of tokens <script type="math/tex">% <![CDATA[
x^{<t'>} %]]></script> is being processed by a bidirectional RNN which acts as an <em>encoder</em>-network by encoding the chunk as set of features <script type="math/tex">% <![CDATA[
a^{<t'>} = (\overrightarrow{a}^{<t'>}, \overleftarrow{a}^{<t'>}) %]]></script> (one feature per word). Note that <script type="math/tex">t'</script> denotes the time step for the current chunk whereas <script type="math/tex">t</script> denotes the time step over the whole sequence.</p>

<p>Those features are then weighted by weights <script type="math/tex">% <![CDATA[
\alpha^{<t, t'>} %]]></script> which must sum up to 1. Those weights indicate how much <em>attention</em> the model should pay to the specific feature (therefore the term <em>attention</em> model). The weighted features are then summed up to form a context <script type="math/tex">% <![CDATA[
c^{<t>} %]]></script>. A different context is calculated for each time step <script type="math/tex">t</script> with different weights. All the contexts are then processed by an unidirectional <em>decoder</em>-RNN which makes the actual predictions <script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script></p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/attention-model.png" alt="attention model">
	<figcaption>Example of an attention model</figcaption>
</figure>

<p>The attention weights can be learned as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\begin{equation}
\alpha^{<t,t'>} = \frac{ \exp( e^{<t,t'>} ) }
{ \sum_{t'=1}^{T_x} \exp( e^{<t,t'>} ) }
\label{attention_model_e}
\end{equation} %]]></script>

<p>Note that the above formula <script type="math/tex">\ref{attention_model_e}</script> only makes the attention weights sum up to 1. The actual attention weights are in the parameter <script type="math/tex">e</script>, which is a trainable parameter that is learned by the decoder network, which can be trained by a very small neural network itself:</p>

<p><img src="/assets/img/articles/ml/dl_5/attention-model-e.png" alt="Many-to-many architecture"></p>

<p>The figure below shows an example for an attention model as well as the calculation of the attention weights inside a single step.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/attention-model-step.png" alt="attention model step">
	<figcaption>Example of an attention model (left) and calculation of the attention weights in a single time step (right) (Credits: Coursera)</figcaption>
</figure>

<p>The magnitude of the different attentions during processing can further be visualized:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/attention-model-weight-visualized.png" alt="attention model">
	<figcaption>Visualized attentions of an attention model (Credits: Coursera)</figcaption>
</figure>

<p>To sum up, here are the most important parameters in an attention model</p>

<table>
  <tbody>
    <tr>
      <td>Symbol</td>
      <td>description</td>
      <td>calculation</td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
a^{<t'>} %]]></script></td>
      <td>feature for the decoder network at chunk-timestep <script type="math/tex">t'</script>
</td>
      <td><script type="math/tex">% <![CDATA[
a^{<t'>} = (\overrightarrow{a}^{<t'>}, \overleftarrow{a}^{<t'>}) %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
\alpha^{<t,t'>} %]]></script></td>
      <td>amount of attention <script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script> should pay to chunk-feature <script type="math/tex">% <![CDATA[
a^{<t'>} %]]></script> at time step <script type="math/tex">t</script> (<script type="math/tex">% <![CDATA[
\sum_{t'} \alpha^{<t,t'>} = 1 %]]></script>)</td>
      <td><script type="math/tex">% <![CDATA[
\alpha^{<t,t'>} = \frac{\exp( e^{<t,t'>} ) }{ \sum_{t'=1}^{T_x} \exp( e^{<t,t'>} ) } %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} %]]></script></td>
      <td>context for the decoder network at time step <script type="math/tex">t</script>
</td>
      <td><script type="math/tex">% <![CDATA[
c^{<t>} = \sum_{t'} \alpha^{<t,t'>} a^{<t'>} %]]></script></td>
    </tr>
    <tr>
      <td><script type="math/tex">% <![CDATA[
\hat{y}^{<t>} %]]></script></td>
      <td>prediction of decoder network at time step <script type="math/tex">t</script>
</td>
      <td> </td>
    </tr>
  </tbody>
</table>

<p>The advantage of an attention model is that it does not process individual words one by one, but rather pays different degrees of attention to different parts of a sentence during processing. This makes them a good fit for tasks like machine translation or image captioning. On the downside the model takes quadratic time to train because for <script type="math/tex">T_x</script> input tokens and <script type="math/tex">T_y</script> output tokens the number of trainable parameters is <script type="math/tex">T_x\cdot T_y</script> (i.e. it has quadratic cost).</p>

<p>If you want to know more about Attention Models, there is a wonderful <a href="https://distill.pub/2016/augmented-rnns/">explanation on Distill</a>.</p>

<h2 id="speech-recognition">Speech recognition</h2>

<p>The problem in speech recognition is that there is usually much more input than output data. Take for example the sentence “<em>The quick brown fox jumps over the lazy dog.</em>” which consists of 35 letters. An audio clip of a recording of this sentence which 10s length and was recorded with a sampling rate of 100Hz (100 samples per second) however has 1000 input samples! The samples of an audio clip can be visualized using a spectrogram.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/spectrogram.png" alt="example of a spectrogram">
	<figcaption>Example of a spectrogram (Credits: Coursera)</figcaption>
</figure>

<p>Traditional approaches to get the transcription for a piece of audio involved aligning individual sounds (<em>phonemes</em>) with the audio signal. For this, a phonetic transcription of the text using the <a href="http://www.internationalphoneticalphabet.org/ipa-sounds/ipa-chart-with-sounds/">International Phonetic alphabet (IPA)</a> was needed. The alignment process then involved detecting individual phonemes in the audio signal and matching them up with the phonetic transcription. To do this, <a href="https://en.wikipedia.org/wiki/Hidden_Markov_model">Hidden Markov Models (HMM)</a> were often used. This method was state of the art for a long time, however it requires an exact phonetic transcription of the speech and is prone to features of the audio signal (e.g. sampling rate, background noise, number of speakers), the text being spoken (intonation, pronunciation, language, speaking rate) or the speaker itself (pitch of the voice, gender, accent).</p>

<p>A more recent approach for speech recognition is a technique called <a href="ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf"><strong>Connectionist Temporal Classification (CTC)</strong></a>. In contrast to HMM, CTC does not need an exact phonetic transcription of the speech audio (i.e. it is <em>alignment-free</em>). The CTC method allows for directly transforming an input signal using an RNN. This is constrasting to the HMM approach where the transcript first had to be mapped to a phonetic translation and the audio signal was then mapped to the individual phonemes. The whole process allows the RNN to output a sequence of characters that is much shorter than the sequence of input tokens.</p>

<p>The underlying principle of CTC is that the input (i.e. spectrograms) are each mapped not only to a single character, but to all characters of the alphabet at the same time (with different probabilities). This may output in a sequence of characters like this:</p>

<p><code class="highlighter-rouge">ttt_h_eee_____ _____q___...</code></p>

<p>This process usually generates a whole bunch of possible output sequences, which can then be further reduced by passing it through a CTC-cost function, which collapses repeated characters. By doing this, we get a set of possible transcriptions, which can then be evaluated by means of a language model, which yields the most likely sequence to form the transcript for the audio.</p>

<p>The main advantage of CTC is that it not only outpeforms all previous models but also that it does not require an intermediate phonetic transcription (i.e. it is an <em>end-to-end</em> model). If you want to know more about CTC, read the <a href="ftp://ftp.idsia.ch/pub/juergen/icml2006.pdf">original paper</a> or pay a visit to <a href="https://distill.pub/2017/ctc/">this wonderful explanation ond Distill</a>.</p>

<h3 id="trigger-word-detection">Trigger word detection</h3>

<p>A special application of speech recognition is trigger word detection, where the focus lies on detecting certain words inside an audio signal to <em>trigger</em> some action. Such systems are widely used in mobile phones or home speakers to wake up the device and make it listen to further instructions.</p>

<p>To train such a system the label for the signal can be simplified by marking it as 0 for time slots where the trigger word is not being said an 1 right after the trigger word was said. Usually a row of ones are used to prevent the amount of zeros being overly large and also because the end of the trigger word might not be easy to define exactly.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/trigger-word-detection.png" alt="trigger word detection">
	<figcaption>Trigger word detection</figcaption>
</figure>

<p>The labelling of input data in trigger word detection can also be illustrated by visualizing the audio clip’s spectrogram together with the <script type="math/tex">y</script>-labels. The following figure contains the spectrogram of an audio clip containing the words <em>“innocent”</em> and <em>“baby”</em> as well as the activation word <em>“activate”</em>.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_5/trigger-word-spectrogram.png" alt="trigger word spectrogram">
	<figcaption>Trigger word spectrogram</figcaption>
</figure>
</div>
  
    
    <p id="disqus_thread"></p>
    <script>

        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

        var disqus_config = function () {
            this.page.url = "https://tiefenauer.github.io";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = "/ml/deep-learning/5"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://tiefenauer.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
</noscript>


</div>
      </div>
   </div>
<!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">© 2018 <a href="http://www.tiefenauer.info">Daniel Tiefenauer</a>
      <!-- Powered by <a href="http://jekyllrb.com">Jekyll</a> with adapted <a href="https://github.com/brianmaierjr/long-haul">Long Haul</a> Theme -->
      </p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/daniel.tiefenauer">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://twitter.com/danitiefenauer">
                  <svg id="twitter" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/tiefenauer">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="https://www.linkedin.com/in/danieltiefenauer">
                <svg id="linkedin" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"></path>
                </svg>
            </a></li>
             
            
            <li><a href="https://www.xing.com/profile/Daniel_Tiefenauer/cv">
                <svg id="xing" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M14.887 24l-5.324-9.667 8.07-14.333h4.933l-8.069 14.333 5.27 9.667h-4.88zm-7.291-19h-4.939l2.768 4.744-4.115 7.256h4.914l4.117-7.271-2.745-4.729z"></path>
                </svg>
            </a></li>
                         
            
            <li><a href="mailto:daniel@tiefenauer.info">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewbox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div>
<!-- end .footer -->


   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>

<!-- Bootstrap: http://getbootstrap.com/docs/4.1/components/alerts/ -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>

<!-- MathJax: https://www.mathjax.org/ -->
<!-- Turn on equation numbering: http://docs.mathjax.org/en/latest/tex.html#automatic-equation-numbering -->
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- Hypothes.is (https://web.hypothes.is/) -->

    <script src="https://hypothes.is/embed.js" async></script>


<!-- Bootstrap scripts-->
<script>
    $('.alert').alert()
</script>

</body>

</html>
