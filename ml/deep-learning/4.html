<!DOCTYPE html>
<!--[if lt IE 7]><html class="no-js lt-ie9 lt-ie8 lt-ie7"><![endif]-->
<!--[if IE 7]><html class="no-js lt-ie9 lt-ie8" <![endif]-->
<!--[if IE 8]><html class="no-js lt-ie9" <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->

<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <title>Deep Learning (4/5): Convolutional Neural Networks</title>

    <!-- Open Graph Meta -->
    <meta content="Dani's Braindump" property="og:site_name">
    
      <meta content="Deep Learning (4/5): Convolutional Neural Networks" property="og:title">
    
    
      <meta content="article" property="og:type">
    
    
      <meta content="My thoughts about the world" property="og:description">
    
    
      <meta content="https://tiefenauer.github.io/ml/deep-learning/4" property="og:url">
    
    
    
      <meta content="https://tiefenauer.github.io/assets/img/header_image.jpg" property="og:image">
    

    <!-- Twitter Cards -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@danitiefenauer">
    <meta name="twitter:creator" content="@danitiefenauer">
    
      <meta name="twitter:title" content="Deep Learning (4/5): Convolutional Neural Networks">
    
    
      <meta name="twitter:url" content="https://tiefenauer.github.io/ml/deep-learning/4">
    
    
      <meta name="twitter:description" content="My thoughts about the world">
    
    
      <meta name="twitter:image:src" content="https://tiefenauer.github.io/assets/img/header_image.jpg">
    


    <!-- Favicon -->
    <link rel="icon" type="image/x-icon" href="/assets/img/favicon.ico" />

    <!-- Come and get me RSS readers -->
    <link rel="alternate" type="application/rss+xml" title="Dani's Braindump" href="https://tiefenauer.github.io/feed.xml" />

    <!-- Bootstrap: include before theme CSS so styles are overridden -->
    <!--<link rel="stylesheet" href="/assets/css/bootstrap.css">-->

    <!-- FontAwesome icons -->
    <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.0.13/css/all.css" integrity="sha384-DNOHZ68U8hZfKXOrtjWvjxusGo9WQnrNx2sqG0tfsghAvtVlRW3tvkXWZh58N9jp" crossorigin="anonymous">

    <!-- Stylesheet -->
    <link rel="stylesheet" href="/assets/css/style.css">
    <!-- custom styles -->
    <link rel="stylesheet" href="/assets/css/custom.css">

    <!--[if IE 8]><link rel="stylesheet" href="/assets/css/ie.css"><![endif]-->
    <link rel="canonical" href="https://tiefenauer.github.io/ml/deep-learning/4">

    <!-- Modernizr -->
    <script src="/assets/js/modernizr.custom.15390.js" type="text/javascript"></script>



     <!-- Google Analytics: change UA-XXXXX-X to be your site's ID. -->
<script>
   (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

ga('create', 'UA-56931568-3', 'auto');
ga('send', 'pageview');

</script>
 
</head>


<body>

    <div class="header">
     <div class="container">
         <h1 class="logo"><a href="/">Dani's Braindump</a></h1>
         <nav class="nav-collapse">
             <ul class="noList">
                 
                 <li class="element first  ">
                     <a href="/index.html">Home</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/about">About</a>
                 </li> 
                 
                 <li class="element  current ">
                     <a href="/ml">Machine Learning</a>
                 </li> 
                 
                 <li class="element   ">
                     <a href="/frenzy">Swiss Frenzy</a>
                 </li> 
                 
                 <li class="element   last">
                     <a href="/contact">Contact</a>
                 </li> 
                 
             </ul>
         </nav>
     </div>
 </div><!-- end .header -->


   <div class="content">
      <div class="container">
         <div class="post">
    

    <h1 class="postTitle">Deep Learning (4/5): Convolutional Neural Networks</h1>

    
        <p class="meta"><span class="time">42</span> Minute Read</p>
    

    
        <p class="alert alert-primary alert-dismissible fade show" role="alert">
    This page uses <strong><a href="https://web.hypothes.is/" target="_blank">Hypothes.is</a></strong>. You can annotate or highlight text directly on this page by expanding the bar on the right. If you find any errors, typos or you think some explanation is not clear enough, please feel free to add a comment. This helps me improving the quality of this site. <strong>Thank you!</strong>
    <button type="button" class="close" data-dismiss="alert" aria-label="Close">
        <span aria-hidden="true">&times;</span>
    </button>
</p>
    

    
        <p style="text-align:center">

    <span class="badge badge-secondary">CONV-Layer</span>

    <span class="badge badge-secondary">POOL-Layer</span>

    <span class="badge badge-secondary">FC-Layer</span>

    <span class="badge badge-secondary">Max- And Avg-Pooling</span>

    <span class="badge badge-secondary">Kernel</span>

    <span class="badge badge-secondary">Filter</span>

    <span class="badge badge-secondary">Channels</span>

    <span class="badge badge-secondary">Padding & Stride</span>

    <span class="badge badge-secondary">Parameter Sharing</span>

    <span class="badge badge-secondary">LeNet-5</span>

    <span class="badge badge-secondary">AlexNet</span>

    <span class="badge badge-secondary">VGG-16</span>

    <span class="badge badge-secondary">Residual Networks (ResNets)</span>

    <span class="badge badge-secondary">1x1 Convolutions</span>

    <span class="badge badge-secondary">Inception Networks</span>

    <span class="badge badge-secondary">Object Localization</span>

    <span class="badge badge-secondary">Landmark And Object Detection</span>

    <span class="badge badge-secondary">Sliding Windows</span>

    <span class="badge badge-secondary">YOLO-Algorithm</span>

    <span class="badge badge-secondary">Intersection Over Union (IoU)</span>

    <span class="badge badge-secondary">Non-Max Suppression</span>

    <span class="badge badge-secondary">Anchor- & Bounding-Boxes</span>

    <span class="badge badge-secondary">Region Proposal (R-CNN)</span>

    <span class="badge badge-secondary">One-Shot Learning</span>

    <span class="badge badge-secondary">Face Recognition</span>

    <span class="badge badge-secondary">Face Verification</span>

    <span class="badge badge-secondary">Similarity Function</span>

    <span class="badge badge-secondary">Siamese Networks</span>

    <span class="badge badge-secondary">Triplet Loss</span>

    <span class="badge badge-secondary">Neural Style Transfer (NST)</span>

    <span class="badge badge-secondary">Content- & Style Cost</span>

</p>
    

    
        <p class="intro">
    <span class="dropcap">I</span>
    <p>n this course you get to know more about Convolutional Neural Networks (CNN, or <em>ConvNet</em>). Because CNNs are often used in computer vision, the key concepts are often illustrated with image processing problems. The course contains a few case studies as well as practical advices for using ConvNets.</p>

</p>
    

    <div><h4 class="no_toc" id="table-of-contents">Table of Contents</h4>

<ul id="markdown-toc">
  <li><a href="#course-overview" id="markdown-toc-course-overview">Course overview</a></li>
  <li><a href="#cnn-in-computer-vision" id="markdown-toc-cnn-in-computer-vision">CNN in computer vision</a>    <ul>
      <li><a href="#convolution-by-example" id="markdown-toc-convolution-by-example">Convolution by example</a></li>
      <li><a href="#padding" id="markdown-toc-padding">Padding</a></li>
      <li><a href="#stride" id="markdown-toc-stride">Stride</a></li>
      <li><a href="#convolution-over-volumes" id="markdown-toc-convolution-over-volumes">Convolution over volumes</a></li>
      <li><a href="#conv-layers" id="markdown-toc-conv-layers">CONV-Layers</a></li>
      <li><a href="#pool-layers" id="markdown-toc-pool-layers">POOL-Layers</a></li>
      <li><a href="#fc-layers" id="markdown-toc-fc-layers">FC-Layers</a></li>
      <li><a href="#advantages-of-convolutional-layers" id="markdown-toc-advantages-of-convolutional-layers">Advantages of convolutional layers</a></li>
    </ul>
  </li>
  <li><a href="#case-studies" id="markdown-toc-case-studies">Case studies</a>    <ul>
      <li><a href="#classic-networks" id="markdown-toc-classic-networks">Classic Networks</a>        <ul>
          <li><a href="#lenet-5" id="markdown-toc-lenet-5">LeNet-5</a></li>
          <li><a href="#alexnet" id="markdown-toc-alexnet">AlexNet</a></li>
          <li><a href="#vgg-16" id="markdown-toc-vgg-16">VGG-16</a></li>
        </ul>
      </li>
      <li><a href="#residual-networks" id="markdown-toc-residual-networks">Residual Networks</a></li>
      <li><a href="#1x1-convolutions" id="markdown-toc-1x1-convolutions">1x1 convolutions</a></li>
      <li><a href="#inception-modules" id="markdown-toc-inception-modules">Inception modules</a></li>
      <li><a href="#computational-cost" id="markdown-toc-computational-cost">Computational cost</a></li>
      <li><a href="#practical-advices-for-using-convnets" id="markdown-toc-practical-advices-for-using-convnets">Practical advices for using ConvNets</a></li>
      <li><a href="#state-of-computer-vision" id="markdown-toc-state-of-computer-vision">State of Computer Vision</a></li>
    </ul>
  </li>
  <li><a href="#detection-algorithms" id="markdown-toc-detection-algorithms">Detection algorithms</a>    <ul>
      <li><a href="#object-localization-ol" id="markdown-toc-object-localization-ol">Object localization (OL)</a></li>
      <li><a href="#landmark-detection" id="markdown-toc-landmark-detection">Landmark detection</a></li>
      <li><a href="#object-detection" id="markdown-toc-object-detection">Object detection</a></li>
      <li><a href="#yolo" id="markdown-toc-yolo">YOLO</a>        <ul>
          <li><a href="#improvements-to-yolo" id="markdown-toc-improvements-to-yolo">Improvements to YOLO</a>            <ul>
              <li><a href="#intersection-over-union" id="markdown-toc-intersection-over-union">Intersection over union</a></li>
              <li><a href="#non-max-supppression" id="markdown-toc-non-max-supppression">Non-max supppression</a></li>
              <li><a href="#anchor-boxes" id="markdown-toc-anchor-boxes">Anchor boxes</a></li>
              <li><a href="#putting-it-all-together" id="markdown-toc-putting-it-all-together">Putting it all together</a></li>
            </ul>
          </li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#face-recognition" id="markdown-toc-face-recognition">Face Recognition</a>    <ul>
      <li><a href="#one-shot-learning" id="markdown-toc-one-shot-learning">One-shot learning</a></li>
      <li><a href="#siamese-networks" id="markdown-toc-siamese-networks">Siamese networks</a></li>
      <li><a href="#triplet-loss" id="markdown-toc-triplet-loss">Triplet loss</a>        <ul>
          <li><a href="#implications-of-tlf" id="markdown-toc-implications-of-tlf">Implications of TLF</a></li>
        </ul>
      </li>
    </ul>
  </li>
  <li><a href="#neural-style-transfer" id="markdown-toc-neural-style-transfer">Neural Style Transfer</a>    <ul>
      <li><a href="#content-cost" id="markdown-toc-content-cost">Content Cost</a></li>
      <li><a href="#style-cost" id="markdown-toc-style-cost">Style Cost</a></li>
    </ul>
  </li>
</ul>

<h2 id="course-overview">Course overview</h2>
<p>The <strong>first week</strong> explains the advantages of CNN and illustrates convolution by example of edge detection in images. You will get to know the different layers that make the difference between an ordinary NN and a CNN. In the programming assignments you implement the key steps for a CNN that can recognize sign language.
In the <strong>second week</strong> you get to know a few classic NN-architectures. You learn about the problems of very deep CNNs and how ResNets can help. Finally you are given some practical advides for using ConfNets in context of computer vision. In this week’s programming assignment you will get to know Keras as a high-level DL-Framework that uses TensorFlow. You will implement a ResNet that is able to detect from a pictureof a person’s face whether a person is happy or not.
<strong>Week three</strong> is about detection algorithms. You learn how a CNN can not only classify but also localize objects inside an image. The programming assignment in the third week is all about autonomous driving. You will implement a YOLO-Model that can detect vehicles inside a picture, state their positions and even classify them as buses or cars.
The <strong>last week</strong> introduces face recognition as a DL problem for CNN. Additionally you get to know Neural Style Transfer (NST) as a special application of CNNs. In the last week you will implement a CNN that is able to generate art images from photos (Neural Style Transfer) and also a face recognition system that can identify people.</p>

<h2 id="cnn-in-computer-vision">CNN in computer vision</h2>

<p>ConvNets are widely used in computer vision. The possible appliances reach far beyond image classification. ConvNets have been trained not only to detect different objects inside a picture, but also classify the object, producing a statement about the image composition (image captioning) or generating new images by learning from artwork (Neural Style Transfer). Such CNN usually require lots training data, as do most NN. However, in constrast to a conventional NN, a CNN must (or should) be able to cope with big, high-resolution images. Processing such data witha  conventional NN would not be feasible for several reasons:</p>

<ul>
  <li>The weight matrices to optimize would become huge</li>
  <li>computational power needed to calculate the optimal weights would be too high</li>
  <li>the required training data would be too large</li>
</ul>

<p>CNN reduce these pain-points by using special operations (<strong>convolution</strong> and <strong>pooling</strong>).</p>

<h3 id="convolution-by-example">Convolution by example</h3>

<p>A convolution layer can generally be understood as a layer which transformation an input volume to an output volume of different size, as shown below:</p>

<p><img src="/assets/img/articles/ml/dl_4/conv_nn.png" alt="convolution" /></p>

<p>A convolution operation reduces an image (or generally: a previous layer) by applying a <strong>filter</strong> (a.k.a. <em>kernel</em>). This filter is a matrix that is being moved step by step over the image. In each step, all the elements of the image matrix that are being covered by the filter are multiplied with the corresnponding elements in the filter matrix. The products are then added up and the filter is moved into the next position. This process is repeated until all the pixels have been captured. A convolution operation is usually denoted by an asterisk <code class="highlighter-rouge">*</code>.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/convolution.gif" alt="Convolution animation" />
	<figcaption>Convolution operation animated (Credits: <a href="https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59" target="_blank">Hackernoon</a>)</figcaption>
</figure>

<p>As you can see from above animation, the convolution operation results in a smaller matrix when no padding is used (we’ll talk about padding later). The parameters in the filter determine what feature is detected. For example consider the following picture (big matrix) and the filter (small matrix). High values in the image matrix mean brighter colors, and low values darker colors. By multiplying with the filter weights and adding the products up we get high values in a single convolution step if the values are big in the left partof the covered area and small on the right side. This filter is therefore able to detect vertical edges where the pixels on the left are bright and the pixels on the left and dark pixels on the right.</p>

<figure>
         	<img src="/assets/img/articles/ml/dl_4/edge_detection.png" alt="edge detection" />
         	<figcaption>Edge detection by example (Credits: Coursera)</figcaption>
         </figure>

<p>This example illustrates how the kernel encompasses a specific features. In the above examples vertical edges could be detected regardless of whether the bright pixels are on the left or right by using absolute values. Similarly, horizontal edges or generally edges of arbitrary angle could be detected with a suitable filter. However, the kernels are not usually set by hand but rather learned by the network. They can detect far more than straight edges, especially in deeper layer when the detected features can get very complex. We will see this in a later example.</p>

<h3 id="padding">Padding</h3>

<p>In the above example the convolution operation resulted in a new matrix which was smaller than the input matrix. This is also called <strong>valid convolution</strong>. Assuming the input image was a matrix of dimensions <script type="math/tex">(n \times n)</script> and the filter a matrix of dimensions <script type="math/tex">(f \times f)</script> then the size of the matrix after convolution could be calculated with the following formula:</p>

<script type="math/tex; mode=display">\begin{equation}
(n - f + 1) \times (n - f + 1)
\label{valid_convolution}
\end{equation}</script>

<p>CNNs usually contain several convolution layers. However though, if we apply additional convolutions, the resulting matrices would become smaller and smaller. Additionally, we would lose the information from the matrix entries at the edges. This behavior is often not desired and can be corrected by using <strong>padding</strong>. By using padding we add additional <script type="math/tex">p</script> pixels (more generally: matrix entries) around the image matrix. The values of those matrix entries can either be set to zero (<strong>zero-padding</strong>) or calculated by some other logic (e.g. average of the neighboring pixels). The following picture shows an example for zero-padding.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/zero_padding.png" alt="zero-padding (example)" />
	<figcaption>Zero-Padding (example for p=2) (Credits: Coursera)</figcaption>
</figure>

<p>If we use a frame of <script type="math/tex">p</script> pixels width for padding, formula <script type="math/tex">\ref{valid_convolution}</script> can be rewritten to the following formula to calculate the new matrix size.</p>

<script type="math/tex; mode=display">\begin{equation}
(n + 2p - f + 1) \times (n + 2p - f + 1)
\label{padding_convolution}
\end{equation}</script>

<p>We can choose <script type="math/tex">p</script> so that the resulting matrix has the same size as the input matrix. This is called <strong>same convolution</strong>. The formula to calculate <script type="math/tex">p</script> for <em>same convolution</em> is:</p>

<script type="math/tex; mode=display">\begin{equation}
p = \frac{f-1}{2}
\label{same_convolution}
\end{equation}</script>

<h3 id="stride">Stride</h3>

<p>Beside the hyperparameter <script type="math/tex">p</script> for padding there is another hyperparameter <script type="math/tex">s</script> for the <strong>stride</strong>. This value defines the step size to use in each convolution operation, i.e. the number of entries to move the filter in each step (<em>step size</em>). In the above examples we have assumed a value of <script type="math/tex">s=1</script>. We can expand formula <script type="math/tex">\ref{padding_convolution}</script> to include an arbitrary stride as follows:</p>

<script type="math/tex; mode=display">\begin{equation}
\left\lfloor \frac{(n + 2p - f + 1)}{s} + 1 \right\rfloor \times \left\lfloor \frac{(n + 2p - f + 1)}{s} + 1 \right\rfloor
\label{stride_convolution}
\end{equation}</script>

<p>Note that rounding down the values is needed to prevent the resulting image size to take on fractional values.</p>

<h3 id="convolution-over-volumes">Convolution over volumes</h3>

<p>We have seen how convolution works for two-dimensional data. This would work for greyscale image, because they only contain one color channel. Usually images contain color in three channels (red, green and blue). Therefore the corresponding image matrices would also be three-dimensional (one two-dimensional matrix for each color channel).</p>

<p>Applying the convolution operation on three-dimensional matrices can be done by using a filter which is also three-dimensional. Generally speaking, in a convolution layer the filter dimensions should match the dimensions of the input matrices. This means if we apply a <script type="math/tex">5\times 5</script>-filter onto a <script type="math/tex">14 \times 14 \times 16</script> matrix the filter’s dimension are by convention <script type="math/tex">( 5 \times 5 \times 16 )</script>. Such a filter can detect different features for the individual color channels. The convolution operation however works as seen above by moving the filter over the image, multiplying the elements and summing the products up. The resulting matrix is then again a two-dimensional matrix. However, we could apply more than one filter and stack the resulting matrices to get a multidimensional output matrix.</p>

<h3 id="conv-layers">CONV-Layers</h3>

<p>The first type of layer a CNN can have is the <strong>convolution layer</strong>, which applies the convolution operation as seen above. A convolution layer <script type="math/tex">l</script> can therefore apply one or more filters to an input matrix. This gives us the following parameters for the layers:</p>

<ul>
  <li><script type="math/tex">f^{[l]}</script>: Kernel size</li>
  <li><script type="math/tex">p^{[l]}</script>: Padding</li>
  <li><script type="math/tex">s^{[l]}</script>: Stride</li>
  <li><script type="math/tex">n_C^{[l]}</script> Number of filters to apply</li>
  <li><script type="math/tex">( n_H^{[l-1]} \times n_W^{[l-1]} \times n_C^{[l-1]} )</script>: Size of the input matrix</li>
  <li><script type="math/tex">( f^{[l]} \times f^{[l]} \times n_C^{[l-l]} )</script>: Size of a matrix for a single kernel</li>
  <li><script type="math/tex">( f^{[l]} \times f^{[l]} \times n_C^{[l-l]} \times n_C^{[l]} )</script>: Size of a matrix for all kernels</li>
  <li><script type="math/tex">( n_H^{[l]} \times n_W^{[l]} \times n_C^{[l]} )</script>: Size of the output matrix for a single training sample</li>
  <li><script type="math/tex">( m \times n_H^{[l]} \times n_W^{[l]} \times n_C^{[l]} )</script>: Size of the output matrix for a single training sample</li>
</ul>

<p>The values of <script type="math/tex">n_H^{[l]}</script> and <script type="math/tex">n_W^{[l]}</script> can be calculated by the corresponding values from the previous layer:</p>

<script type="math/tex; mode=display">n^{[l]} = \left\lfloor \frac{n^{[l-1]} + 2p^{[l]} - f}{s} \right\rfloor</script>

<p>Several convolutional layers can be combined to a <strong>Deep-CNN</strong>. The hyperparameters (size of the input or output matrix, number of filters, size of the filters, etc…) can be tuned with the following rules of thumb:</p>

<ul>
  <li>the size of the input matrices (and therefore also the size of the output matrix should become smaller with increasing layer depth</li>
  <li>the number of channels should also decrease with each increasing layer depth</li>
</ul>

<h3 id="pool-layers">POOL-Layers</h3>

<p>The second layer type in a CNN is the <strong>pooling layer</strong>. This layer is similar to the CONV layer in that a filter is slid over the input matrix. However, instead of multiplying the elements and summing up, the values for the output matrix are determined in a different way. There are two sub-types of this layer:</p>

<ul>
  <li><strong>Max-Pooling (POOL)</strong>: The value in the output matrix corresponds to the maximum value in the covered area</li>
  <li><strong>Average-Pooling (AVG)</strong>: The value in the output matrix corresponds to the average value in the covered area</li>
</ul>

<figure>
	<img src="/assets/img/articles/ml/dl_4/max_avg_pooling.png" alt="Max- and Avg-Pooling (example)" />
	<figcaption>Max- and Avg-Pooling (example) (Credits: Coursera)</figcaption>
</figure>

<p>The pooling operation can be performed similar to the convolution operation by using different values for the stride. However, in contrast to the convolution operation, padding is usually not applied for pooling layers. Since there are no weights in the pooling filter there are no parameters to learn. Therefore, pooling layers usually reduce the input image in one or more dimensions. This is intentional because that way the CNN is forced to keep only the most important parameters.</p>

<h3 id="fc-layers">FC-Layers</h3>

<p>Usually when talking about the number of layers in a CNN only the layers with parameters (weights) are taken into account. Therefore, a pooling layer does not add to the network depth and can be considered belonging to the previous layer.</p>

<p>The last layer in a CNN is usually a layer of dimensions <script type="math/tex">(k \times 1)</script> with all neurons connected to the previous layer (<strong>fully connected</strong>). This layer type is the same like we used in all NN so far. If neccessary, this vector can further be reduced to a <script type="math/tex">(t \times 1)</script> vector (<script type="math/tex">% <![CDATA[
t < k %]]></script>) by multiplying it with a <script type="math/tex">(t \times k)</script> matrix.</p>

<p>A CNN usually contains several CONV and POOL layers. A series of CONV layers is often followed by a single POOL layer. A typical CNN architecture is therefore the following:</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CONV -&gt; CONV -&gt; ... -&gt; POOL -&gt; CONV -&gt; CONV -&gt; ... -&gt; POOL -&gt; FC -&gt; FC -&gt; ... -&gt; FC -&gt; Softmax
</code></pre></div></div>

<h3 id="advantages-of-convolutional-layers">Advantages of convolutional layers</h3>

<p>CONV layers have some advantages over FC layers:</p>

<ul>
  <li><strong>Parameter sharing</strong>: A CONV layer needs to optimize less parameters than a FC layer because a lot of the parameters are shared. A feature detector that is useful in one part of the image is probably also useful in another part of the image.</li>
</ul>
<figure>
	<img src="/assets/img/articles/ml/dl_4/shared_weights.png" alt="shared weights" />
	<figcaption>Shared weights (parameter sharing)</figcaption>
</figure>
<ul>
  <li><strong>Sparsity of Connections</strong>: A value in the output matrix of a CONV layer only depends from a subset of the values in the input matrix. Therefore when performing backprop, a lot of parameters can be set to zero. This simplifies the calculation</li>
</ul>

<p>Because a CNN has far less parameters to optimize than a CNN, a CNN also needs far less training data than a comparable Deep-NN without convolutional layers.</p>

<h2 id="case-studies">Case studies</h2>
<p>Looking at existing network architectures are a good opportunity to see how others have designed their NN to solve a specific task. This can come in handy for problems where the results are transferreable (e.g. in Computer Vision).</p>

<h3 id="classic-networks">Classic Networks</h3>
<p>There are a few noteworthy CNN architectures that have had a big impact on Computer Vision or DL in general:</p>

<ul>
  <li>LeNet-5</li>
  <li>AlexNet</li>
  <li>VGG-16</li>
</ul>

<h4 id="lenet-5">LeNet-5</h4>
<p>LeNet-5 is a CNN with the following architecture:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/lenet-5.png" alt="LeNet-5 architecture" />
	<figcaption>LeNet-5 architecture (Credits: Coursera)</figcaption>
</figure>

<p>LeNet-5 was trained on the MNIST dataset, a collection of hand-written digits. This CNN is quite old and comparably small to current CNN (approximately 60k trainable parameters). It uses sigmoid or tanh as activation functions in the hidden layers. However, it does not use softmax as classifier in the last layer whereas today we probably would. We can further notice that it only uses valid convolutions (i.e. no padding) which results in the matrices becoming smaller and smaller.</p>

<h4 id="alexnet">AlexNet</h4>
<p>AlexNet is a CNN with the following architecture:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/alexnet.png" alt="AlexNet architecture" />
	<figcaption>AlexNet architecture (Credits: Coursera)</figcaption>
</figure>

<p>We can observe that AlexNet was trained on color images because it uses several channels in the input layer. Its architecture is similar to LeNet-5, but it is much bigger (approximately 60M trainable Parameters) and uses a so. It also uses ReLU as activation functions in the hidden layers and softmax as the classifier in the last layer. Its performance was far better than LeNet-5 which was an inspiration for scientists to use DL for computer vision.</p>

<h4 id="vgg-16">VGG-16</h4>

<p>VGG-16 was a 16-Layer CNN with approximately 138M trainable parameters. The convolution layer all used SAME-convolution. Therefore the architecture is comparably simple (i.e. uniform and systematic). The following image shows a simplified representation of the 16 layers:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/vgg-16.png" alt="VGG-16 architecture" />
	<figcaption>VGG-16 architecture (Credits: Coursera)</figcaption>
</figure>

<h3 id="residual-networks">Residual Networks</h3>
<p>Recent Networks have become very deep. The CNN <a href="https://blogs.microsoft.com/ai/microsoft-researchers-win-imagenet-computer-vision-challenge/">Microsoft used to win the ImageNet competition in 2015</a> was as deep as 152 layers! We have seen in <a href="/ml/deep-learning/1#initialization">part 2</a> that such a network usually suffers from vanishing (or in rare cases exploding) gradients and thus the gradient for the earlier layers decreases to zero very rapidly as training proceeds:</p>

<p><img src="/assets/img/articles/ml/dl_4/vanishing_grad.png" alt="vanishing gradient" /></p>

<p>In order to train such very deep CNN that don’t suffer from exploding/vanishing gradient, we use special building blocks known as <strong>residual blocks</strong>. Those residual blocks consist of two conventional layer together with a shortcut (called <strong>skip-connection</strong>):</p>

<p><img src="/assets/img/articles/ml/dl_4/residual-block.png" alt="skip connection" /></p>

<p>This residual block uses the activation of the previous layer <script type="math/tex">a^{[l]}</script> for the calculation of its activation in the second layer. This calculation can be formally defined as:</p>

<script type="math/tex; mode=display">a^{[l+2]} = g(W^{[l+2]} g(W^{[l+1]} a^{[l]} + b^{[l+1]}) + b^{[l+2]} + a^{[l]})</script>

<p>If the weights inside the residual blocks become very small because of vanishing gradients, the activation of the previous layer dominates over the cell state during the calculation of the activation of the second layer. Therefore skip connections allow the forward propagation in a ResNet to learn some kind of <strong>identity function</strong> if the weights become too small. This makes learning the optimal parameters much simpler. If the input activation <script type="math/tex">a^{[l]}</script> has the same dimensions as the output activation <script type="math/tex">a^{[l+2]}</script> we call te residual block an <strong>identity block</strong>:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/identity-block.png" alt="identity block" />
	<figcaption><strong>Identity block</strong>: Skip connection "skips over" 2 layers (Credits: Coursera)</figcaption>
</figure>

<p>The upper path is the <em>shortcut path</em>, the lower the <em>main path</em>. However, the dimensions of the input and output activations do not neccessarily need to be the same. This is often the case when a skip-connection spans over more than 2 layers. In such cases we need the shortcut path to resize the input activation accordingly by applying a convolution operation to the input activation. Since the only purpose of this convolution is matching up the dimensions, the shortcut path does not use any non-linear activation function.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/convolution-block.png" alt="convolution block" />
	<figcaption><strong>Convolution block</strong>: Skip connection "skips over" 3 layers (Credits: Coursera)</figcaption>
</figure>

<p>By stacking several of those residual blocks we get a ResNet that does not suffer from vanishing gradients anymore. In such a ResNet the danger of overfitting becomes much smaller. I.e. the error on the training data will gradually decrease whereas in contrast to conventional CNN it would increas again after a certain point.</p>

<h3 id="1x1-convolutions">1x1 convolutions</h3>
<p>It can sometimes make sense to use a convolution layer with a kernel of dimensions <script type="math/tex">( 1 \times 1 \times n_C^{[l]})</script> (wherer <script type="math/tex">n_C^{[l]}</script> denotes the number of channels in the input layer). Such a layer can be used to reduce (or increase) the number of channels while preserving all the other dimensions. Generally, <script type="math/tex">n_C^{[l+1]}</script> (the number of channels in the output layer) corresponds to the number of 1x1-filters applied to the input layer and can be set to any value. This is why 1x1 convolution layer are sometimes also referred to <strong>networks inside a network</strong>.</p>

<h3 id="inception-modules">Inception modules</h3>

<p>CONV- and POOL-operations can be applied simultaneiously in an <strong>inception module</strong>. The results of the individual operations can be stacked to appear as separate channels in the next layer.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/inception-module-simple.png" alt="simple inception module" />
	<figcaption>simple inception module (Credits: Coursera)</figcaption>
</figure>

<p>The values for <script type="math/tex">n_H</script> and <script type="math/tex">n_W</script> in the next layer are the same, but there are more channels. To achieve this, the POOL-layers must apply padding (which is usually only done for CONV-layers).</p>

<p>An inception module can also apply multiple CONV- and/or POOL-operations in a row to compute a batch of channels:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/inception-module-complex.png" alt="complex inception module" />
	<figcaption>complex inception module (Credits: Coursera)</figcaption>
</figure>

<h3 id="computational-cost">Computational cost</h3>
<p>CNN however (especially CNN using inception modules) often require extremely high computational cost, because each element of the input layer needs to be multiplied with each element of a filter. Consider the following example of a convolution layer:</p>

<p><img src="/assets/img/articles/ml/dl_4/computational-cost-before.png" alt="computational cost before 1x1 convolution" /></p>

<p>If we apply 32 convolution filters of dimensions <script type="math/tex">(5 \times 5 \times 192)</script> each to an input layer of dimensions <script type="math/tex">(28 \times 28 \times 192)</script> the number of multiplication operations is <script type="math/tex">(28 \cdot 2 \cdot 32) \cdot (5 \cdot 5 \cdot 32) \approx 120M</script>. To reduce this computational cost 1x1 convolutions are applied to reduce the input layer to a lower-dimensional intermediate matrix and apply the convolution to this intermediate matrix:</p>

<p><img src="/assets/img/articles/ml/dl_4/computational-cost-after.png" alt="computational cost after 1x1 convolution" /></p>

<p>By using the intermediate matrix the number of operations for the first step is reduced to <script type="math/tex">(28 \cdot 28 \cdot 16) \cdot 192 \approx 2.4M</script> and for the second step to <script type="math/tex">(28 \cdot 28 \cdot 32) \cdot (5 \cdot 5 \cdot 18) \approx 10M</script> (total: <script type="math/tex">\approx 12.4M</script> operations).</p>

<p>Surprisingly the predictive performance of the CNN does not suffer significantly by this reduction of computational cost. Multiple inception modules can be combined to form an inception network this way without the danger of exponentially increasing computational cost.</p>

<h3 id="practical-advices-for-using-convnets">Practical advices for using ConvNets</h3>

<p>It is sometimes useful to start out from the results of other researchers. Often however it is difficult to re-build a CNN from a paper from scratch. Luckily, the authors provide the pre-trained models and/or labelled data sets to train on for download. You can (and should) use those models for <a href="/ml/deep-learning/3#transfer-learning">transfer learning</a>. Additionally, if you only have little own training data to train on, you can use <a href="/ml/deep-learning/2#data-augmentation">data augmentation</a> to synthethizsize additional training data.</p>

<h3 id="state-of-computer-vision">State of Computer Vision</h3>

<p>Because for a lot of tasks in CV there is only very little labelled training data available, careful hand-engineering (of the CNN architecture, the features or other components) is very important to still get a reasonable data. This is different from other tasks where you can rely on a lot of training data to improve performance and spending a lot of time on hand-engineering might not be the way to go. The absence of abundant training data for very complex tasks might be the reason that some very complex network architectures have evolved especially in the area of computer vision.
After training, the performance of a trained CNN can be verified on a benchmark dataset. A lot of competitions on <a href="https://www.kaggle.com/">Kaggle</a> or other platforms are about beating an existing benchmark. Winning in such compoetitions might get you a lot of attention. There are some useful techniques to help beating the crowd in such competitions (although those are usually not applied in production because they are usually too expensive):</p>

<ul>
  <li><strong>Ensembling</strong>: Sometimes a good result on a benchmark dataset does not mean the CNN will apply well on unknown data. A (rarely used) possibility in such cases would be to train several (e.g. 3-15) CNN independently from another and then use the mean value of their outputs (not their weights!) to calculate the result.</li>
  <li><strong>Multi-Crop at test time</strong>: A similar approach to ensembling is applying multi-cropping. Instead of training several networks and averaging their outputs you generate several variants of your image and average the results of a single network on each of the variants. In multi-cropping you crop your image to get the center piece, then move the cropping area to the corners to get the corner areas. This way you will get 5 crops out of your original image. You can mirror the image and apply the same process to get another five crops (<strong>10-crop-method</strong>). You can then classify each of the crops and calculate the average result of your network to get a prediction.
<img src="/assets/img/articles/ml/dl_4/multi-crop.png" alt="10-crop" /></li>
</ul>

<h2 id="detection-algorithms">Detection algorithms</h2>

<p>Besides classifying an image (i.e. labelling a picture with its content) it is often important to know where exactly on the picture the labelled object is. This is called <strong>object localization</strong>. Most of the time however we have more than one object on a single picture and not only want to know what objects there are but also where they are located. This is called <strong>object detection</strong>. Object detection is especially important in problem areas like autonomous driving where we usually label multiple objects (pedestrians, other cars, signs, red lights, etc.) inside an image and also want to know where they are.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/object-localization.png" alt="object localization and detection" />
	<figcaption>Object localization and detection (Credits: Coursera)</figcaption>
</figure>

<h3 id="object-localization-ol">Object localization (OL)</h3>

<p>To localize an object on an image we can use a <strong>bounding box</strong> with the following parameters:</p>

<ul>
  <li><script type="math/tex">b_x, b_y \in [0, 1]</script>: coordinates of the center of the bounding box (with the zero point being at the top left corner of the image)</li>
  <li><script type="math/tex">b_H, b_W \in [0, 1]</script>: height and width of the bounding box (as ratios of the image size, i.e. a value of <script type="math/tex">b_W = 0.3</script> means the box’ with is 30% of the image width)</li>
  <li><script type="math/tex">p_c</script>: binary value whether the object is inside the picture or not</li>
</ul>

<figure>
	<img src="/assets/img/articles/ml/dl_4/bounding-box.png" alt="definition of a bounding box" />
	<figcaption>Definition of a bounding box (Credits: Coursera)</figcaption>
</figure>

<p>So far we used a one-hot representation to encode each object. The parameters of the bounding box can now be added to this encoding to get the vector representation of a single object:</p>

<script type="math/tex; mode=display">\begin{equation}
y = \begin{pmatrix}
p_c \\
b_x \\
b_y \\
b_H \\
b_W \\
c_1 \\
... \\
c_n
\end{pmatrix}
\label{label_vector}
\end{equation}</script>

<p>Because the components <script type="math/tex">y_2 .. y_n</script> of this vector are only relevant if the first component <script type="math/tex">y_1 = p_c = 1</script> we can redefine our cost function as follows:</p>

<script type="math/tex; mode=display">% <![CDATA[
\mathcal{L} =
\begin{cases}
\sum_{i=1}^n (\hat{y}_i - y_i)^2 & \text{if } y_1 = 1 \\
(\hat{y}_1 - y_1)^2 & \text{if } y_1 = 0
\end{cases} %]]></script>

<p>Note: This formula is the definition of the mean squared error loss function but also applies to other loss functions.</p>

<h3 id="landmark-detection">Landmark detection</h3>

<p>Knowing an objects position on an image may be enough for applications like autonomous driving. However, for other problems this might not be enough. In face recognition for example it is not only important to know where the eyes, the nose, the mouth etc… are located, but also specific points of those body parts (e.g. the corners of the eyes). Such points are called <strong>landmarks</strong> and finding them is called <strong>landmark detection</strong> accordingly. It is important to notice that when working with landmarks detection the labelled landmarks have to be consistent over the training instances (e.g. landmark 1 for the left corner of the left eye, landmark 2 for the right corner of the left eye, and so on)</p>

<h3 id="object-detection">Object detection</h3>

<p>In order to not only localize known objects but also find objects inside images one can work with <strong>sliding windows</strong>. A sliding window is successively moved over an image to analyze a part of the image in each step. While doing this the step size needs to be small enough so that an object is not skipped. The window size can be increased gradually to detect objects that would otherwise be too big to fit into the analyzed area.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/sliding-windows.png" alt="object detection with sliding windows" />
	<figcaption>Object detection with sliding windows (Credits: Coursera)</figcaption>
</figure>

<p>This method of object recognition however is computationally very expensive and not feasible. A better implementation of a sliding window is its <strong>convolutional implementation</strong>. This involves turing fully connected layers into convolutional layers. To do this, consider the following example of a ConvNet with two FC layers:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/sliding-windows-conv-before.png" alt="ConvNet with FC layers" />
	<figcaption>ConvNet with FC layers (Credits: Coursera)</figcaption>
</figure>

<p>We can replace each of these FC layers with equivalent CONV layers by applying a filter that has the same dimensions as the input matrices. By using several of these filters (in this example: 400) we can create a CONV layer that is equivalent to the FC layer and has the same dimensionality in the third dimension</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/sliding-windows-conv-after.png" alt="ConvNet with FC layres replaced by CONV layers" />
	<figcaption>ConvNet with FC layers replaced by CONV layers (Credits: Coursera)</figcaption>
</figure>

<p>This method of replacing FC layers with equivalent CONV layers can be used to implement the sliding windows method by using convolution. Consider the following example of a ConvNet with an input image of size <script type="math/tex">14 \times 14 \times 3</script> (note that the third dimension has been left out in the drawing for simplicity). This image is now convoluted by applying a <script type="math/tex">5 \times 5</script> filter, some max-pooling and further convolution layers.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/sliding-windows-conv-1.png" alt="example of a ConvNet" />
	<figcaption>Example of a ConvNet (Credits: Coursera)</figcaption>
</figure>

<p>We can implement the sliding window method now by adding a border of two pixels to the left. This results in an input image of dimensions <script type="math/tex">16 \times 16 \times 3</script>. We can now apply the same <script type="math/tex">5 \times 5</script> convolution and all the pooling and other pooling layers like before. However, because of the changed dimensions of the input image, the dimensions of the intermediate matrices are also different.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/sliding-windows-conv-2.png" alt="convolutional implementation of sliding window" />
	<figcaption>Convolutional implementation of sliding window (Credits: Coursera)</figcaption>
</figure>

<p>It turns out however that the upper left corner of the resulting matrix (blue square) is the result of the upper left area of the input image (blue area). The other square in the output matrix correspond to the other areas in the input image accordingly. This mapping applies to the intermediate matrices too. The advantage calculating the result convolutionally is that the forward propagations of all areas are combined into one form sharing a lot of the computation in the regions of the image that are common. In contrast, with the sliding window method as described above forward propagation would have been done several times independently (once for each position of the sliding window). A convolutional implementation of sliding windows has therefore considerable lower computational cost compared to a sequential implementation because all the positions of a sliding window are computed in a single forward pass.</p>

<h3 id="yolo">YOLO</h3>

<p>One disadvantage of the sliding windows method (even with a convolutional implementation) is that the window may not come to lie over an object nicely. That is there are only positions of the sliding window where parts of the object are outside the border of the window. Therefore the ConvNet cannot (or only badly) detect the object. One approach to solve this is the <strong>YOLO algorithm</strong> (<em>You Only Look Once</em>). This algorithm divides the image into multiple cells of equal size.</p>

<p><img src="/assets/img/articles/ml/dl_4/yolo.png" alt="YOLO segmentation" /></p>

<p>The basic idea is to then apply the sliding window process as described above to each of the cells. For this there must be labels for each grid cell encoded as label vectors as described in <script type="math/tex">\ref{label_vector}</script>. Whether an object belongs to a specific image segment or not is determined by the coordinates of the center of the bounding box. That way an object can only ever belong to exactly one segment.</p>

<p>In contrast to previously seen CNN the output for YOLO-networks is a volume, not a two-dimensional matrix. If we have for example 3 classes, the label vector would consist of 8 elements (<script type="math/tex">p_c, b_x, b_y, b_H, b_W, c_1, c_2, c_3</script>). If we divide the image into 9 segments as in the picture above we can detect one object per segment resulting in a label matrix (and therefore also the output matrix <script type="math/tex">Y</script> of the CNN) of dimensions <script type="math/tex">( 3 \times 3 \times 8 )</script>. The following picture shows an example of a CNN that uses YOLO to divide an image into <script type="math/tex">19 \times 19</script> segments and distinguishes between 80 classes and shows how it encodes an image:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/yolo-architecture.png" alt="Encoding architecture for YOLO" />
	<figcaption>Encoding architecture for YOLO (Credits: Coursera)</figcaption>
</figure>

<p>Note that the coordinates <script type="math/tex">b_x, b_y</script> are relative to the grid cell, not the image. Therefore their values need to lie between 0 and 1. Likewise, the size of the bounding box (specified by <script type="math/tex">b_H</script> and <script type="math/tex">b_W</script>) is relative to the size of the grid cell, but since the bounding box may overlap several grid cells those values can become greater than 1 (whereas in a different algorithm without segmentation the size of the bounding box cannot be greater than than the image).</p>

<p>The YOLO algorithm is a convolutional implementation of the sliding window method which makes it performant by sharing a lot of the computation. In fact, the YOLO algorithm is so performant that it can be used for real-time object detection.</p>

<h4 id="improvements-to-yolo">Improvements to YOLO</h4>

<p>There are a few ways to improve the performance of the YOLO algorithm.</p>

<ul>
  <li>Intersection over union</li>
  <li>Non-max suppression</li>
  <li>anchor boxes</li>
</ul>

<h5 id="intersection-over-union">Intersection over union</h5>
<p>An improvement of the YOLO algorithm can be achieved by calculating how much a predicted bounding box overlaps with the actual bounding box.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/iou.png" alt="Intersection over Union" />
	<figcaption>Intersection over Union (IoU) (Credits: Coursera, with adjustments)</figcaption>
</figure>

<p>This value is called <strong>Intersection over Union (IoU)</strong> and can be more formally defined as:</p>

<script type="math/tex; mode=display">\begin{equation}
IoU = \frac{ \text{size of the overlapping part (intersection)} }{ \text{size of the combined bounding boxes (union)} }
\end{equation}</script>

<p><img src="/assets/img/articles/ml/dl_4/iou-formula.png" alt="IoU formula" /></p>

<p>The IoU can be used to evaluate the accuracy of the localization. When <script type="math/tex">IoU > 0.5</script> (or any reasonable value) the localization could be marked as <em>correct</em>.</p>

<h5 id="non-max-supppression">Non-max supppression</h5>
<p>YOLO can further be improved by using <strong>non-max suppression (NMS)</strong>. NMS prevents YOLO from detecting the same object multiple times, e.g. if the YOLO algorithm calculates several bounding boxes for the same object with their center coordinates assigned to different cells. It does so by assigning the first component <script type="math/tex">p_c</script> of a label vector (see <script type="math/tex">\ref{label_vector}</script>) a value between 0 and 1 (whereas before this value was binary, i.e. either 0 <em>or</em> 1). The value of <script type="math/tex">p_c</script> corresponds to the confidence of the algorithm that the bounding box contains the respective object. NMS then searches for overlapping boxes and removes all but the one with the highest value of <script type="math/tex">p_c</script>. The following image illustrates this (the blue boxes are the ones that are removed, the green boxes are the ones that are retained):</p>

<p><img src="/assets/img/articles/ml/dl_4/nms.png" alt="NMS" /></p>

<p>The algorithm for NMS is as follows:</p>

<ol>
  <li>Delete all bounding with <script type="math/tex">p_c</script> below a threshold value (e.g. 0.6)</li>
  <li>While there are bounding boxes:
    <ul>
      <li>Take the bounding box with the largest <script type="math/tex">p_c</script>. Output this as a prediction</li>
      <li>Find all bounding boxes overlapping this bounding box (<script type="math/tex">IoU > 0.5</script>) and remove them</li>
    </ul>
  </li>
</ol>

<h5 id="anchor-boxes">Anchor boxes</h5>
<p>To enable the YOLO algorithm to detect more than one object per cell you can use <strong>anchor boxes</strong>. An anchor box corresponds to a bounding box for a certain object that overlaps the bounding box of another object. An object is assigned to the anchor box with the highes IoU value (whereas previously the object was assigned to the grid cell that contained the object’s midpoint). To predict more than one object per cell, the label-vectors need to be stacked on top of each other in the output. The following picture illustrates this for two anchor boxes (i.e. a YOLO-NN that can detect two objects per cell).</p>

<p><img src="/assets/img/articles/ml/dl_4/anchor-boxes.png" alt="Anchor boxes" /></p>

<h5 id="putting-it-all-together">Putting it all together</h5>

<p>Assume we train a ConvNet using YOLO by dividing the images with a <script type="math/tex">3 \times 3</script> grid whereas the ConvNet should be able to detect 2 objects per grid cell and we have 3 possible classes of objects. The dimensions of the output matrix of such a ConvNet would therefore be <script type="math/tex">3 \times 3 \times (2 \cdot 8) = 3 \times 3 \times 16</script>.</p>

<p>Generally speaking the dimensions of the output <script type="math/tex">y</script> of a ConvNet using YOLO with a <script type="math/tex">k \times l</script> grid, <script type="math/tex">n_c</script> classes and <script type="math/tex">m</script> anchor boxes can be calculated as follows:</p>

<script type="math/tex; mode=display">dim(y) = k \times l \times (m \cdot (5 + n_c))</script>

<p>The term <script type="math/tex">(5 + n_c)</script> stems from the five components needed to indicate the confidence and the coordinates/size of a bounding box (<script type="math/tex">p_c, b_x, b_y, b_H, b_W</script>) together with the <script type="math/tex">n_c</script> components needed to form a one-hot vector of a class.</p>

<p>A CNN that uses YOLO can therefore do the following:</p>

<ol>
  <li>divide the picture into a grid of equally sized segments by using the <strong>YOLO-algorithm</strong></li>
  <li>detect and locate several objects in an image by applying a convolutional implementation of a <strong>sliding window</strong></li>
  <li>calculate the <strong>bounding boxes</strong> by training on the labelled data</li>
  <li>use <strong>anchor-boxes</strong> to detect multiple different objects within the same grid cell</li>
  <li>apply <strong>non-max suppression</strong> to keep only one bounding box per object</li>
  <li>output a matrix of dimensions <script type="math/tex">dim(y)</script> which represents the <strong>encoding of the information</strong></li>
</ol>

<p>If we visualize the output matrix from the last step it might look something like this:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/yolo-output-example.png" alt="Example of a CNN applying YOLO" />
	<figcaption>Example of a CNN applying YOLO (19x19 grid, multiple classes, non-max-suppression) (Credits: Coursera)</figcaption>
</figure>

<h2 id="face-recognition">Face Recognition</h2>
<p>One possible appliance of a CNN or Computer Vision in general is the <strong>face recognition</strong>. In face recognition we want to identify a person from a database of <script type="math/tex">K</script> persons, i.e. we want a single input image to map to the ID of one of the <script type="math/tex">K</script> persons in the database (or no output if the person was not recognized). This is different from <strong>face verification</strong> where we compare the input image only to a single persona and verify whether the input image is that of the claimed person.</p>

<h3 id="one-shot-learning">One-shot learning</h3>
<p>Up to this point we have only seen CNN that needed a lot of pictures to be trained. However, because we usually don’t have a lot of pictures of the same person, the problem with face recognition is that a CNN needs to be trained that is able to identify a person based on just a single picture. This process is called <strong>one-shot learning</strong>. Conventional CNN are not suitable for this kind of task, not only because they require a huge amount of training data, but also because the whole network would need to be re-trained if we want to identify a new person.</p>

<p>Instead when performing face recognition we apply a <strong>similarity function</strong> <script type="math/tex">d(x^{(i)} , x^{(j)})</script> that is able to calculate the (dis)similarity between two images of persons <script type="math/tex">x^{(i)}</script> and <script type="math/tex">x^{(j)}</script> as a value <script type="math/tex">\tau</script> (<em>degree of difference</em>). <script type="math/tex">\tau</script> is small for persons who look alike and large for different persons:</p>

<script type="math/tex; mode=display">% <![CDATA[
d(x^{(i)} , x^{(j)})
\begin{cases}
\leq \tau & \text{"same"} \\
\gt \tau & \text{"different"}
\end{cases} %]]></script>

<h3 id="siamese-networks">Siamese networks</h3>
<p>One way to implement this similarity function is a <strong>siamese network</strong>. Such a network encodes an input image as a vector of arbitrary dimensions (e.g. 128 components). The network can be understood as a function <script type="math/tex">f(x)</script> that encodes an image <script type="math/tex">x</script> whereas similar pictures lead to similar encodings.</p>

<p><img src="/assets/img/articles/ml/dl_4/siamese-network-function.png" alt="Siamese Network function" /></p>

<p>The similarity function can then be implemented as the vector norm of two image vectors:</p>

<script type="math/tex; mode=display">d(x^{(i)} , x^{(j)}) = \lVert f(x^{(i)}) - f(x^{(j)})\rVert^2_2</script>

<h3 id="triplet-loss">Triplet loss</h3>
<p>A siamese network should calculate similar image vectors for similar images and different vectors for different images. In other words: the distance between image vectors should be small for similar images and big for dissimilar images. We need to train the siamese network to exhibit this property. To do this we can use the <strong>triplet loss function (TLF)</strong>. When using the TLF we define the image of one specific Person as anchor image <script type="math/tex">A</script> and compare it with another image of the same person (positive image <script type="math/tex">P</script>) and an image of a different person (negative image <script type="math/tex">N</script>). Because of the initially formulated condition the following equation needs to hold true:</p>

<script type="math/tex; mode=display">d(A,P) = \lVert f(A) - f(P) \rVert^2_2 \leq \lVert f(A) - f(N) \rVert^2_2 = d(A,N)</script>

<p>We can rearrange this equation and get:</p>

<script type="math/tex; mode=display">\lVert f(A) - f(P) \rVert^2_2 - \lVert f(A) - f(N) \rVert^2_2 \leq 0</script>

<p>However, there a catch with this equation: We could achieve it to be true by simply “calculating” the zero vector for each image! To prevent this, we add a parameter <script type="math/tex">\alpha</script> and get:</p>

<script type="math/tex; mode=display">\lVert f(A) - f(P) \rVert^2_2 - \lVert f(A) - f(N) \rVert^2_2 + \alpha \leq 0</script>

<p>By rearranging it back to the original form we get:</p>

<script type="math/tex; mode=display">\lVert f(A) - f(P) \rVert^2_2 + \alpha \leq \lVert f(A) - f(N) \rVert^2_2</script>

<p>The parameter <script type="math/tex">\alpha</script> is also called <strong>margin</strong>. The effect of this margin is that the value of <script type="math/tex">\tau</script> for pictures of the same person differs a lot from pictures of different persons (i.e. <script type="math/tex">d(A,P)</script> is separated from <script type="math/tex">d(A,N)</script> by a big margin).</p>

<p><img src="/assets/img/articles/ml/dl_4/tlf-distance-matrix.png" alt="TLF distance matrix" /></p>

<p>Considering all the points mentioned above we can define the TLF as follows:</p>

<script type="math/tex; mode=display">\mathcal{L}(A,P,N) = max(\lVert f(A) - f(P) \rVert^2_2 - \lVert f(A) - f(N) \rVert^2_2 + \alpha, 0)</script>

<p>Maximizing the two values prevents the network from calculating negative losses. The total cost can be calculated as usuall by summing the losses over all triplets:</p>

<script type="math/tex; mode=display">J = \sum_{i=1}^m \mathcal{L}(A^{(i)},P^{(i)},N^{(i)})</script>

<h4 id="implications-of-tlf">Implications of TLF</h4>

<p>The definition of the TLF function implies that in order to train a siamese network that exhibits the required properties we need at least two different images of the same person. To ensure a strong discrimination we should also consider triplets <script type="math/tex">(A,P,N)</script> where <script type="math/tex">N</script> is the image of a person who looks similar to <script type="math/tex">A</script>. That way we force the network to also learn to differentiate “hard” cases.</p>

<p>An alternative approach for facce recognition is to treat it as a binary classification problem. This could be used for an acces control system based on face recognition. We could store precomputed image vectors in a database and would only have to calculate/compare a person’s image vector. We can do this by training a CNN which calculates a value close to 1 for pictures of the same person and a value close to 0 for pictures of different persons. The calculation of this value could be as follows:</p>

<script type="math/tex; mode=display">\hat{y} = \sigma \left( \sum_{k=1}^K w_i \cdot \vert f( x^{(i)}_k ) - f( x^{(j)}_k ) \vert + b \right)</script>

<p>We could alternatively use the <strong>Chi-Squared-Similarity</strong></p>

<script type="math/tex; mode=display">\hat{y} = \sigma \left( \sum_{k=1}^K w_i \cdot \frac{\left( f( x^{(i)}_k ) - f( x^{(j)}_k \right)^2}{f( x^{(i)}_k ) + f( x^{(j)}_k} \right)</script>

<h2 id="neural-style-transfer">Neural Style Transfer</h2>

<p>Another interesting task in the field of Computer Vision is <strong>Neural Style Transfer (NST)</strong>. NST takes a style image <script type="math/tex">S</script> (e.g. a painting) and applies its style to a content image <script type="math/tex">C</script> to produce a new image <script type="math/tex">G</script>. Because a new image is generated, a model that persforms NST is called a <strong>generative model</strong>.</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/nst.png" alt="Neural Style Transfer" />
	<figcaption>Examples for Neural Style Transfer (Credits: Coursera, images by Justin Johnson)</figcaption>
</figure>

<p>We can train such a model by training a NN that uses two cost functions:</p>

<ul>
  <li><script type="math/tex">J_{CONTENT}(C,G)</script>: Cost regarding the content of the original content image <script type="math/tex">C</script> and the generated image <script type="math/tex">G</script> (<strong>content cost function</strong>)</li>
  <li><script type="math/tex">J_{STYLE}(S,G)</script>: Cost regarding the style of the original style image <script type="math/tex">S</script> and the generated image <script type="math/tex">G</script> (<strong>style cost function</strong>)</li>
</ul>

<p>Both cost function can be combined to a single cost function:</p>

<script type="math/tex; mode=display">J(G) = \alpha \cdot J_{CONTENT}(C,G) + \beta \cdot J_{STYLE}(S,G)</script>

<p>This cost function can be minimized the same way as in regular NN. To do this, the image <script type="math/tex">G</script> is initialized with random noise and then optimized by applying gradient descent to minimize the costs.</p>

<h3 id="content-cost">Content Cost</h3>

<p>To understand how the content cost function works we can visualize what a deep NN is learning by inspecting the activations of neurons in different layers. By visualizing the image patches that maximally activate a neuron we can get a sense for what the neurons are learning. It turns out that the NN usually learns abstract things like “<em>vertical edges</em>”, “<em>bright/dark</em>” etc. in higher layers and more complex things like “<em>water</em>”, “<em>dogs</em>” etc. in deeper layers:</p>

<figure>
	<img src="/assets/img/articles/ml/dl_4/nn_layers.png" alt="NN layers" />
	<figcaption>Visualization of what is learned in different layers of a NN (Credits: Coursera)</figcaption>
</figure>

<p>We can calculate the content cost at any layer in the network and thus control how big its influence is on the generated image. Let’s for example consider an image of a dog as the content image <script type="math/tex">C</script>. If we calculate the content cost in a higher layer we force the network to generate an image which looks similar to <script type="math/tex">C</script>. If we calculate the content cost in deeper layers we allow the network to generate an almost arbitrary image as long as there is a dog in the image. Usually some hidden layer in the middle is chosen to achieve a good balance between content and style.</p>

<p>Let <script type="math/tex">a^{[l](C)}</script> and <script type="math/tex">a^{[l](G)}</script> be the activations of layer <script type="math/tex">l</script> for the content image <script type="math/tex">C</script> or the style image <script type="math/tex">S</script> respectively. The CCF can now be defined as the element-wise square difference:</p>

<script type="math/tex; mode=display">J_{CONTENT}^{[l]}(C,G) = \frac{1}{2} \lVert a^{[l](C)} - a^{[l](G)} \rVert^2</script>

<h3 id="style-cost">Style Cost</h3>
<p>To calculate the similarity between the styles of two images we can define style as the correlation between the activation across channels in a layer <script type="math/tex">l</script>. This correlation can be understood as follows:</p>

<ul>
  <li><strong>High correlation</strong>: An image patch which has a high value in both channel A and channel B contains a style property in both channels</li>
  <li><strong>Low correlation</strong>: An image patch which ahs a high value in channel A and a small value in channel B (or vice versa) contains a style properties of channel A, but not the properties of channel B.</li>
</ul>

<p>Let’s visualize this with an example. Consider the two high-level style properties “contains vertical lines” and “has an orange tint” which are reflected in different channels. If the two properties are highly correlated it means the original style image <script type="math/tex">G</script> often contains vertical lines in conjunction with an orange tint. We can therefore measure the similarity in style of the generated image <script type="math/tex">G</script> by checking if the correlation between these properties (channels) is high too.</p>

<p>This can be expressed more formal as follows: Let <script type="math/tex">a^{[l]}_{i,j,k}</script> be the activation of neuron <script type="math/tex">i</script> in layer <script type="math/tex">l</script> for the pixel at position <script type="math/tex">j,k</script> in the style image <script type="math/tex">S</script>. We can represent the correlation between the <script type="math/tex">n_C</script> channels in this layer as a style matrix G (a.k.a. <strong>Gram-Matrix</strong>) which has the dimensions <script type="math/tex">(n_c \times n_c)</script>. This is easiest if the image matrix is unrolled into a 2-dimensional matrix:</p>

<p><img src="/assets/img/articles/ml/dl_4/nst-unrolling.png" alt="unrolling an image" /></p>

<p>The elements of this matrix can thenbe calculated as follows:</p>

<script type="math/tex; mode=display">G^{[l]}_{kk'} = \sum^{n^{[l]}_H}_{i=1} \sum^{n^{[l]}_W}_{j=1} a^{[l]}_{i,j,k} \cdot a^{[l]}_{i,j,k'}</script>

<p><img src="/assets/img/articles/ml/dl_4/gram-matrix.png" alt="unrolling an image" /></p>

<p>This style matrix can be computed separately for both the style image <script type="math/tex">S</script> and the generated image <script type="math/tex">G</script>. The SCF can then be defined as:</p>

<script type="math/tex; mode=display">J_{STYLE}^{[l]}(S,G) = \lVert G^{[l](S)} - G^{[l](G)} \rVert^2_F
= \frac{1}{s \cdot n_H^{[l]} \cdot n_W^{[l]} \cdot n_C^{[l]}} \cdot \sum_k \sum_{k'} \left( G^{[l](S)}_{kk'} - G^{[l](G)}_{kk'}\right)^2</script>

<p>The SCF can be applied on different layers (low-level and high-level layers) whereas the results can be weighted by appliying a parameter <script type="math/tex">\lambda^{[l]}</script> and summed up to get the overall style cost across all layers:</p>

<script type="math/tex; mode=display">J_{STYLE}(S,G) = \sum_l \lambda^{[l]} \cdot J_{STYLE}^{[l]}(S,G)</script>
</div>
  
    
    <p id="disqus_thread"></p>
    <script>

        /**
         *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
         *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables*/

        var disqus_config = function () {
            this.page.url = "https://tiefenauer.github.io";  // Replace PAGE_URL with your page's canonical URL variable
            this.page.identifier = "/ml/deep-learning/4"; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
        };

        (function() { // DON'T EDIT BELOW THIS LINE
            var d = document, s = d.createElement('script');
            s.src = 'https://tiefenauer.disqus.com/embed.js';
            s.setAttribute('data-timestamp', +new Date());
            (d.head || d.body).appendChild(s);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


</div>
      </div>
   </div><!-- end .content -->

   <div class="footer">
   <div class="container">
      <p class="copy">&copy; 2018 <a href="http://www.tiefenauer.info">Daniel Tiefenauer</a>
      <!-- Powered by <a href="http://jekyllrb.com">Jekyll</a> with adapted <a href="https://github.com/brianmaierjr/long-haul">Long Haul</a> Theme -->
      </p>

      <div class="footer-links"> 
         <ul class="noList"> 
            
            <li><a href="https://www.facebook.com/daniel.tiefenauer">
                  <svg id="facebook-square" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M82.667,1H17.335C8.351,1,1,8.351,1,17.336v65.329c0,8.99,7.351,16.335,16.334,16.335h65.332 C91.652,99.001,99,91.655,99,82.665V17.337C99,8.353,91.652,1.001,82.667,1L82.667,1z M84.318,50H68.375v42.875H50V50h-8.855V35.973 H50v-9.11c0-12.378,5.339-19.739,19.894-19.739h16.772V22.3H72.967c-4.066-0.007-4.57,2.12-4.57,6.078l-0.023,7.594H86.75 l-2.431,14.027V50z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://twitter.com/danitiefenauer">
                  <svg id="twitter" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M99.001,19.428c-3.606,1.608-7.48,2.695-11.547,3.184c4.15-2.503,7.338-6.466,8.841-11.189 c-3.885,2.318-8.187,4-12.768,4.908c-3.667-3.931-8.893-6.387-14.676-6.387c-11.104,0-20.107,9.054-20.107,20.223 c0,1.585,0.177,3.128,0.52,4.609c-16.71-0.845-31.525-8.895-41.442-21.131C6.092,16.633,5.1,20.107,5.1,23.813 c0,7.017,3.55,13.208,8.945,16.834c-3.296-0.104-6.397-1.014-9.106-2.529c-0.002,0.085-0.002,0.17-0.002,0.255 c0,9.799,6.931,17.972,16.129,19.831c-1.688,0.463-3.463,0.71-5.297,0.71c-1.296,0-2.555-0.127-3.783-0.363 c2.559,8.034,9.984,13.882,18.782,14.045c-6.881,5.424-15.551,8.657-24.971,8.657c-1.623,0-3.223-0.096-4.796-0.282 c8.898,5.738,19.467,9.087,30.82,9.087c36.982,0,57.206-30.817,57.206-57.543c0-0.877-0.02-1.748-0.059-2.617 C92.896,27.045,96.305,23.482,99.001,19.428z"></path>
                  </svg>
            </a></li>
            
            
            <li><a href="https://github.com/tiefenauer">
                  <svg id="github" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M79.099,79.099 c-3.782,3.782-8.184,6.75-13.083,8.823c-1.245,0.526-2.509,0.989-3.79,1.387v-7.344c0-3.86-1.324-6.699-3.972-8.517 c1.659-0.16,3.182-0.383,4.57-0.67c1.388-0.287,2.855-0.702,4.402-1.245c1.547-0.543,2.935-1.189,4.163-1.938 c1.228-0.75,2.409-1.723,3.541-2.919s2.082-2.552,2.847-4.067s1.372-3.334,1.818-5.455c0.446-2.121,0.67-4.458,0.67-7.01 c0-4.945-1.611-9.155-4.833-12.633c1.467-3.828,1.308-7.991-0.478-12.489l-1.197-0.143c-0.829-0.096-2.321,0.255-4.474,1.053 c-2.153,0.798-4.57,2.105-7.249,3.924c-3.797-1.053-7.736-1.579-11.82-1.579c-4.115,0-8.039,0.526-11.772,1.579 c-1.69-1.149-3.294-2.097-4.809-2.847c-1.515-0.75-2.727-1.26-3.637-1.532c-0.909-0.271-1.754-0.439-2.536-0.503 c-0.782-0.064-1.284-0.079-1.507-0.048c-0.223,0.031-0.383,0.064-0.478,0.096c-1.787,4.53-1.946,8.694-0.478,12.489 c-3.222,3.477-4.833,7.688-4.833,12.633c0,2.552,0.223,4.889,0.67,7.01c0.447,2.121,1.053,3.94,1.818,5.455 c0.765,1.515,1.715,2.871,2.847,4.067s2.313,2.169,3.541,2.919c1.228,0.751,2.616,1.396,4.163,1.938 c1.547,0.543,3.014,0.957,4.402,1.245c1.388,0.287,2.911,0.511,4.57,0.67c-2.616,1.787-3.924,4.626-3.924,8.517v7.487 c-1.445-0.43-2.869-0.938-4.268-1.53c-4.899-2.073-9.301-5.041-13.083-8.823c-3.782-3.782-6.75-8.184-8.823-13.083 C9.934,60.948,8.847,55.56,8.847,50s1.087-10.948,3.231-16.016c2.073-4.899,5.041-9.301,8.823-13.083s8.184-6.75,13.083-8.823 C39.052,9.934,44.44,8.847,50,8.847s10.948,1.087,16.016,3.231c4.9,2.073,9.301,5.041,13.083,8.823 c3.782,3.782,6.75,8.184,8.823,13.083c2.143,5.069,3.23,10.457,3.23,16.016s-1.087,10.948-3.231,16.016 C85.848,70.915,82.88,75.317,79.099,79.099L79.099,79.099z"></path>
                  </svg>
            </a></li>
             
            
            <li><a href="https://www.linkedin.com/in/danieltiefenauer">
                <svg id="linkedin" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M19 0h-14c-2.761 0-5 2.239-5 5v14c0 2.761 2.239 5 5 5h14c2.762 0 5-2.239 5-5v-14c0-2.761-2.238-5-5-5zm-11 19h-3v-11h3v11zm-1.5-12.268c-.966 0-1.75-.79-1.75-1.764s.784-1.764 1.75-1.764 1.75.79 1.75 1.764-.783 1.764-1.75 1.764zm13.5 12.268h-3v-5.604c0-3.368-4-3.113-4 0v5.604h-3v-11h3v1.765c1.396-2.586 7-2.777 7 2.476v6.759z"/>
                </svg>
            </a></li>
             
            
            <li><a href="https://www.xing.com/profile/Daniel_Tiefenauer/cv">
                <svg id="xing" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(2.0)" d="M14.887 24l-5.324-9.667 8.07-14.333h4.933l-8.069 14.333 5.27 9.667h-4.88zm-7.291-19h-4.939l2.768 4.744-4.115 7.256h4.914l4.117-7.271-2.745-4.729z"/>
                </svg>
            </a></li>
                         
            
            <li><a href="mailto:daniel@tiefenauer.info">
                  <svg id="mail" class="custom-icon" version="1.1" xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink" viewBox="0 0 100 100" style="height: 30px; width: 30px;"><circle class="outer-shape" cx="50" cy="50" r="48" style="opacity: 1;"></circle>
                  <path class="inner-shape" style="opacity: 1;" transform="translate(25,25) scale(0.5)" d="M50,1C22.938,1,1,22.938,1,50s21.938,49,49,49s49-21.938,49-49S77.062,1,50,1z M25.5,25.5h49 c0.874,0,1.723,0.188,2.502,0.542L50,57.544L22.998,26.041C23.777,25.687,24.626,25.499,25.5,25.5L25.5,25.5z M19.375,68.375v-36.75 c0-0.128,0.005-0.256,0.014-0.383l17.96,20.953L19.587,69.958C19.448,69.447,19.376,68.916,19.375,68.375L19.375,68.375z M74.5,74.5 h-49c-0.541,0-1.072-0.073-1.583-0.212l17.429-17.429L50,66.956l8.653-10.096l17.429,17.429C75.572,74.427,75.041,74.5,74.5,74.5 L74.5,74.5z M80.625,68.375c0,0.541-0.073,1.072-0.211,1.583L62.652,52.195l17.96-20.953c0.008,0.127,0.014,0.255,0.014,0.383 L80.625,68.375L80.625,68.375z"></path>
                  </svg>
            </a></li>
            
         </ul>
      </div>
   </div>
</div><!-- end .footer -->


   <!-- Add jQuery and other scripts -->
<script src="//ajax.googleapis.com/ajax/libs/jquery/1.11.2/jquery.min.js"></script>
<script>window.jQuery || document.write('<script src=""><\/script>')</script>
<script src="/assets/js/dropcap.min.js"></script>
<script src="/assets/js/responsive-nav.min.js"></script>
<script src="/assets/js/scripts.js"></script>

<!-- Bootstrap: http://getbootstrap.com/docs/4.1/components/alerts/ -->
<script src="/assets/js/bootstrap.bundle.min.js"></script>

<!-- MathJax: https://www.mathjax.org/ -->
<!-- Turn on equation numbering: http://docs.mathjax.org/en/latest/tex.html#automatic-equation-numbering -->
<script type="text/x-mathjax-config">
        MathJax.Hub.Config({ TeX: { equationNumbers: {autoNumber: "AMS"} } });
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<!-- Hypothes.is (https://web.hypothes.is/) -->

    <script src="https://hypothes.is/embed.js" async></script>


<!-- Bootstrap scripts-->
<script>
    $('.alert').alert()
</script>

</body>

</html>
